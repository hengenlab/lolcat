{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found processed pickle. Loading from '../data/processed/v1_dataset.pkl'.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset, TensorDataset\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "from src import Dataset, FiringRates, ISIDistribution, ConcatFeats\n",
    "from src.network import MLP\n",
    "\n",
    "aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'i', 'i5Htr3a': 'i', 'e4Scnn1a': 'e', 'e4Rorb': 'e',\n",
    "         'e4other': 'e', 'e4Nr5a1': 'e', 'i6Htr3a': 'i', 'i6Sst': 'i', 'e6Ntsr1': 'e',\n",
    "         'i23Pvalb': 'i', 'i23Htr3a': 'i', 'i1Htr3a': 'i', 'i4Sst': 'i', 'e5Rbp4': 'e',\n",
    "         'e5noRbp4': 'e', 'i23Sst': 'i', 'i4Htr3a': 'i', 'i6Pvalb': 'i', 'i5Pvalb': 'i',\n",
    "         'i4Pvalb': 'i'}        \n",
    "dataset = Dataset('../data', force_process=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Split dataset and add correct labels for task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    if dataset.data_source == 'v1':\n",
    "        if dataset.labels_col == 'pop_name':\n",
    "            dataset.drop_dead_cells(cutoff=30)\n",
    "            keepers = ['e5Rbp4', 'e23Cux2', 'i6Pvalb', 'e4Scnn1a', 'i23Pvalb', 'i23Htr3a',\n",
    "             'e4Rorb', 'e4other', 'i5Pvalb', 'i4Pvalb', 'i23Sst', 'i4Sst', 'e4Nr5a1',\n",
    "             'i1Htr3a', 'e5noRbp4', 'i6Sst', 'e6Ntsr1']\n",
    "            dataset.drop_other_classes(classes_to_keep=keepers)\n",
    "            print(np.unique(dataset.cell_type_ids))\n",
    "            if k == '17celltypes':\n",
    "                pass #all filtering done above\n",
    "            elif k == '13celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e23', 'i5Sst': 'i5Sst', 'i5Htr3a': 'i5Htr3a', 'e4Scnn1a': 'e4', 'e4Rorb': 'e4',\n",
    "                         'e4other': 'e4', 'e4Nr5a1': 'e4', 'i6Htr3a': 'i6Htr3a', 'i6Sst': 'i6Sst', 'e6Ntsr1': 'e6',\n",
    "                         'i23Pvalb': 'i23Pvalb', 'i23Htr3a': 'i23Htr3a', 'i1Htr3a': 'i1Htr3a', 'i4Sst': 'i4Sst', 'e5Rbp4': 'e5',\n",
    "                         'e5noRbp4': 'e5', 'i23Sst': 'i23Sst', 'i4Htr3a': 'i4Htr3a', 'i6Pvalb': 'i6Pvalb', 'i5Pvalb': 'i5Pvalb',\n",
    "                         'i4Pvalb': 'i4Pvalb'}        \n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '11celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'Cux2', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'Scnn1a', 'e4Rorb': 'Rorb',\n",
    "                         'e4other': 'other', 'e4Nr5a1': 'Nr5a1', 'i6Htr3a': 'Htr3a', 'i6Sst': 'Sst', 'e6Ntsr1': 'Ntsr1',\n",
    "                         'i23Pvalb': 'Pvalb', 'i23Htr3a': 'Htr3a', 'i1Htr3a': 'Htr3a', 'i4Sst': 'Sst', 'e5Rbp4': 'Rbp4',\n",
    "                         'e5noRbp4': 'noRbp4', 'i23Sst': 'Sst', 'i4Htr3a': 'Htr3a', 'i6Pvalb': 'Pvalb', 'i5Pvalb': 'Pvalb',\n",
    "                         'i4Pvalb': 'Pvalb'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '4celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'e', 'e4Rorb': 'e', 'e4other': 'e', \n",
    "                         'e4Nr5a1': 'e', 'i6Htr3a': 'Htr3a', 'i6Sst': 'Sst', 'e6Ntsr1': 'e', 'i23Pvalb': 'Pvalb', 'i23Htr3a': 'Htr3a',\n",
    "                         'i1Htr3a': 'Htr3a', 'i4Sst': 'Sst', 'e5Rbp4': 'e', 'e5noRbp4': 'e', 'i23Sst': 'Sst', 'i4Htr3a': 'Htr3a',\n",
    "                         'i6Pvalb': 'Pvalb', 'i5Pvalb': 'Pvalb', 'i4Pvalb': 'Pvalb'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '5layers':\n",
    "                aggr_dict = {'e23Cux2': '23', 'i5Sst': '5', 'i5Htr3a': '5', 'e4Scnn1a': '4', 'e4Rorb': '4', 'e4other': '4',\n",
    "                         'e4Nr5a1': '4', 'i6Htr3a': '6', 'i6Sst': '6', 'e6Ntsr1': '6', 'i23Pvalb': '23', 'i23Htr3a': '23',\n",
    "                         'i1Htr3a': '1', 'i4Sst': '4', 'e5Rbp4': '5', 'e5noRbp4': '5', 'i23Sst': '23', 'i4Htr3a': '4',\n",
    "                         'i6Pvalb': '6', 'i5Pvalb': '5', 'i4Pvalb': '4'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '2celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'i', 'i5Htr3a': 'i', 'e4Scnn1a': 'e', 'e4Rorb': 'e',\n",
    "                         'e4other': 'e', 'e4Nr5a1': 'e', 'i6Htr3a': 'i', 'i6Sst': 'i', 'e6Ntsr1': 'e',\n",
    "                         'i23Pvalb': 'i', 'i23Htr3a': 'i', 'i1Htr3a': 'i', 'i4Sst': 'i', 'e5Rbp4': 'e',\n",
    "                         'e5noRbp4': 'e', 'i23Sst': 'i', 'i4Htr3a': 'i', 'i6Pvalb': 'i', 'i5Pvalb': 'i',\n",
    "                         'i4Pvalb': 'i'}    \n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "\n",
    "    # Split into train/val/test sets\n",
    "    dataset.split_cell_train_val_test(test_size=test_size, val_size=val_size, seed=cell_split_seed)\n",
    "    #dataset.split_trial_train_val_test(test_size=0.2, val_size=0.2, temp=True, seed=1234)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=None):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        '''\n",
    "        if log_interval and batch_idx % log_interval == 0:\n",
    "            \n",
    "            print('Train Epoch: {} [({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, 100. * batch_idx / len(train_loader), loss.item()))\n",
    "                '''\n",
    "\n",
    "def test(model, device, loader, tag, labels=dataset.cell_type_labels):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds = []\n",
    "    corrects = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            pred_ = np.ndarray.flatten(pred.cpu().numpy())\n",
    "            targ_ = target.cpu().numpy()\n",
    "            preds.append(pred_)\n",
    "            corrects.append(targ_)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += len(target)\n",
    "    '''\n",
    "    print('{} set: Accuracy: {}/{} ({:.0f}%)'.format(tag,\n",
    "        correct, total,\n",
    "        100. * correct / total))\n",
    "        '''\n",
    "    corrects = np.hstack(corrects)\n",
    "    preds = np.hstack(preds)\n",
    "    acc = f1_score(corrects,preds,average='macro')\n",
    "    #print('F1:',acc)\n",
    "    cm = confusion_matrix(corrects,preds,normalize='true')\n",
    "    return cm,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_viz(X_train,y_train,aug,num_samples=20):\n",
    "    y_sort_ord = np.argsort(y_train)\n",
    "    sorted_Xt = X_train[y_sort_ord]\n",
    "    sorted_yt = y_train[y_sort_ord]\n",
    "    uniq_inds = np.unique(sorted_yt,return_index=True)[1]\n",
    "    for idx, uniq_idx in enumerate(uniq_inds):\n",
    "        sampled_Xt = sorted_Xt[uniq_idx:uniq_idx+num_samples]\n",
    "        np.save('{}_ct{}_aug{}_{}bins_{}samples.npy'.format(k,str(idx),str(aug),str(sampled_Xt.shape[1]),str(num_samples)),sampled_Xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-32efceda325f>:47: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if dist_params!=prev_dist_params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Found processed pickle. Loading from '../data/processed/v1_dataset.pkl'.\n",
      "Load\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16.]\n",
      "[ 0.  0.  0. ... 16. 16. 16.]\n",
      "2\n",
      "2.1\n",
      "2.4\n",
      "3\n",
      "3.1\n",
      "3.2\n",
      "3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-32efceda325f>:47: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if dist_params!=prev_dist_params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Found processed pickle. Loading from '../data/processed/v1_dataset.pkl'.\n",
      "Load\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16.]\n",
      "[ 0.  0.  0. ... 16. 16. 16.]\n",
      "2\n",
      "2.1\n",
      "2.4\n",
      "3\n",
      "3.1\n",
      "3.2\n",
      "3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-32efceda325f>:47: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if dist_params!=prev_dist_params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Found processed pickle. Loading from '../data/processed/v1_dataset.pkl'.\n",
      "Load\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16.]\n",
      "[ 0.  0.  0. ... 16. 16. 16.]\n",
      "2\n",
      "2.1\n",
      "2.4\n",
      "3\n",
      "3.1\n",
      "3.2\n",
      "3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-32efceda325f>:47: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if dist_params!=prev_dist_params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Found processed pickle. Loading from '../data/processed/v1_dataset.pkl'.\n",
      "Load\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16.]\n",
      "[ 0.  0.  0. ... 16. 16. 16.]\n",
      "2\n",
      "2.1\n",
      "2.4\n",
      "3\n",
      "3.1\n",
      "3.2\n",
      "3.3\n"
     ]
    }
   ],
   "source": [
    "hp_df = pd.read_csv('hp_regrid111_v2_augviz.csv') #this is a hyperparameter grid\n",
    "prev_dist_params = np.asarray([0]) #just empty initialization\n",
    "aug = '1'\n",
    "#FOR EVERY HYPERPARAMETER COMBINATION\n",
    "for index,row in hp_df.iterrows(): #just skipping the first row because I ran previously\n",
    "    \n",
    "    #LOAD THE HYPERPARAMETERS\n",
    "    hpc = row['hp_idx']\n",
    "    k,distribution,cell_split_seed,bin_size = row['k'],row['distribution'],int(row['cell_split_seed']),row['bin_size']\n",
    "    isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step,fr_dist_bins_start,fr_dist_bins_stop,fr_dist_bins_step = [row['isi_dist_bins_start'],row['isi_dist_bins_stop'],row['isi_dist_bins_step'],row['fr_dist_bins_start'],row['fr_dist_bins_stop'],row['fr_dist_bins_step']]\n",
    "    lr,n_hiddens = row['lr'],row['n_hiddens']\n",
    "    n_hiddens = [int(nh) for nh in n_hiddens.rsplit(',')]\n",
    "    dist_params = [k,distribution,cell_split_seed,bin_size,isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step,fr_dist_bins_start,fr_dist_bins_stop,fr_dist_bins_step,]\n",
    "    if len(list(set([isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step]))) > 1:\n",
    "        isi_dist_bins = list(np.arange(isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step))\n",
    "    else:\n",
    "        isi_dist_bins = isi_dist_bins_start\n",
    "    if len(list(set([fr_dist_bins_start,fr_dist_bins_stop,fr_dist_bins_step]))) > 1:    \n",
    "        fr_dist_bins = list(range(int(fr_dist_bins_start),int(fr_dist_bins_stop),int(fr_dist_bins_step)))\n",
    "    else:\n",
    "        fr_dist_bins = fr_dist_bins_start    \n",
    "    #SET SOME HARDCODED HYPERPARAMETERS\n",
    "    batch_size = 1\n",
    "    n_class = int(''.join(filter(str.isdigit, k)))\n",
    "    cell_sample_seed = 1\n",
    "    test_size, val_size = 0.2,0.2 \n",
    "    \n",
    "    if 'dropout_p' in hp_df.columns and 'weight_decay' in hp_df.columns:\n",
    "        dropout_p = row['dropout_p']\n",
    "        weight_decay = row['weight_decay']\n",
    "    else:\n",
    "        dropout_p = 0 #currently overfitting\n",
    "        weight_decay = 0\n",
    "    \n",
    "\n",
    "\n",
    "    if 'augmentation_perc1' in hp_df.columns and 'augmentation_perc2' in hp_df.columns:\n",
    "        augmentation_percs = [row['augmentation_perc1'],row['augmentation_perc2']]\n",
    "    else:\n",
    "        augmentation_percs = [0,0]\n",
    "\n",
    "        \n",
    "    epochs = 50 #seems to be enough to converge\n",
    "    threshold = 30 #minimum number of spikes per trial\n",
    "    \n",
    "    #IF THIS HYPERPARAMETER COMBO HAS DIFFERENT DATA (FEATURES OR CLASS LABELS) RELOAD AND RENORMALIZE THE DATA\n",
    "    if dist_params!=prev_dist_params:\n",
    "\n",
    "        print(1)\n",
    "        dataset = Dataset('../data', force_process=False)\n",
    "        dataset.data_source = 'v1'\n",
    "        dataset.labels_col = 'pop_name'\n",
    "        dataset.num_trials_in_window = 33\n",
    "        print('Load')\n",
    "        dataset = load_data()\n",
    "        print(dataset.cell_type_ids)\n",
    "\n",
    "        fr_transform = FiringRates(window_size=3, bin_size=bin_size)\n",
    "        if distribution == 'ISI':\n",
    "            aug1_train_transform = ISIDistribution(bins=isi_dist_bins, min_isi=0, max_isi=0.4, augmentation_percs=[1,0])\n",
    "            aug2_train_transform = ISIDistribution(bins=isi_dist_bins, min_isi=0, max_isi=0.4, augmentation_percs=[0,1])\n",
    "            aug0_train_transform = ISIDistribution(bins=isi_dist_bins, min_isi=0, max_isi=0.4, augmentation_percs=[0,0])\n",
    "        elif distribution == 'FR':\n",
    "            data_transform = fr_transform\n",
    "            train_transform = data_transform\n",
    "            val_test_transform = data_transform\n",
    "        elif distribution == 'ISIFR':\n",
    "            isi_transform = ISIDistribution(bins=isi_dist_bins, min_isi=0, max_isi=0.4)\n",
    "            fr_transform = FiringRates(window_size=3, bin_size=bin_size)\n",
    "            data_transform = ConcatFeats(fr_transform, isi_transform)\n",
    "            train_transform = data_transform\n",
    "            val_test_transform = data_transform\n",
    "        print(2)\n",
    "        print(2.1)\n",
    "        X_train_fr, y_train_fr = dataset.get_set('train', transform=fr_transform)\n",
    "        print(2.4)\n",
    "        train_mask = X_train_fr.sum(axis=1) > threshold\n",
    "        print(3)\n",
    "        print(3.1)\n",
    "        X_train, y_train = dataset.get_set('train',transform=aug0_train_transform)\n",
    "        X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "\n",
    "        aug_viz(X_train,y_train,aug='0',num_samples=20)\n",
    "        print(3.2)\n",
    "        X_train, y_train = dataset.get_set('train',transform=aug1_train_transform)\n",
    "        X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "        aug_viz(X_train,y_train,aug='1',num_samples=20)\n",
    "        print(3.3)\n",
    "        X_train, y_train = dataset.get_set('train',transform=aug2_train_transform)\n",
    "        X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "        aug_viz(X_train,y_train,aug='2',num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
