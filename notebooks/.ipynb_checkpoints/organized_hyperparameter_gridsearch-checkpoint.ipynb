{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "from src import Dataset, ISIDistributionDataset, EdgeDistributionDataset, FRDistributionDataset, ISIFRDistributionDataset\n",
    "from src.network import MLP\n",
    "\n",
    "aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'i', 'i5Htr3a': 'i', 'e4Scnn1a': 'e', 'e4Rorb': 'e',\n",
    "         'e4other': 'e', 'e4Nr5a1': 'e', 'i6Htr3a': 'i', 'i6Sst': 'i', 'e6Ntsr1': 'e',\n",
    "         'i23Pvalb': 'i', 'i23Htr3a': 'i', 'i1Htr3a': 'i', 'i4Sst': 'i', 'e5Rbp4': 'e',\n",
    "         'e5noRbp4': 'e', 'i23Sst': 'i', 'i4Htr3a': 'i', 'i6Pvalb': 'i', 'i5Pvalb': 'i',\n",
    "         'i4Pvalb': 'i'}        \n",
    "dataset = Dataset('../data', force_process=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Split dataset and add correct labels for task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    if dataset.data_source == 'v1':\n",
    "        if dataset.labels_col == 'pop_name':\n",
    "            dataset.drop_dead_cells(cutoff=30)\n",
    "            keepers = ['e5Rbp4', 'e23Cux2', 'i6Pvalb', 'e4Scnn1a', 'i23Pvalb', 'i23Htr3a',\n",
    "             'e4Rorb', 'e4other', 'i5Pvalb', 'i4Pvalb', 'i23Sst', 'i4Sst', 'e4Nr5a1',\n",
    "             'i1Htr3a', 'e5noRbp4', 'i6Sst', 'e6Ntsr1']\n",
    "            dataset.drop_other_classes(classes_to_keep=keepers)\n",
    "            if k == '17celltypes':\n",
    "                pass #all filtering done above\n",
    "            elif k == '13celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e23', 'i5Sst': 'i5Sst', 'i5Htr3a': 'i5Htr3a', 'e4Scnn1a': 'e4', 'e4Rorb': 'e4',\n",
    "                         'e4other': 'e4', 'e4Nr5a1': 'e4', 'i6Htr3a': 'i6Htr3a', 'i6Sst': 'i6Sst', 'e6Ntsr1': 'e6',\n",
    "                         'i23Pvalb': 'i23Pvalb', 'i23Htr3a': 'i23Htr3a', 'i1Htr3a': 'i1Htr3a', 'i4Sst': 'i4Sst', 'e5Rbp4': 'e5',\n",
    "                         'e5noRbp4': 'e5', 'i23Sst': 'i23Sst', 'i4Htr3a': 'i4Htr3a', 'i6Pvalb': 'i6Pvalb', 'i5Pvalb': 'i5Pvalb',\n",
    "                         'i4Pvalb': 'i4Pvalb'}        \n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '11celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'Cux2', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'Scnn1a', 'e4Rorb': 'Rorb',\n",
    "                         'e4other': 'other', 'e4Nr5a1': 'Nr5a1', 'i6Htr3a': 'Htr3a', 'i6Sst': 'Sst', 'e6Ntsr1': 'Ntsr1',\n",
    "                         'i23Pvalb': 'Pvalb', 'i23Htr3a': 'Htr3a', 'i1Htr3a': 'Htr3a', 'i4Sst': 'Sst', 'e5Rbp4': 'Rbp4',\n",
    "                         'e5noRbp4': 'noRbp4', 'i23Sst': 'Sst', 'i4Htr3a': 'Htr3a', 'i6Pvalb': 'Pvalb', 'i5Pvalb': 'Pvalb',\n",
    "                         'i4Pvalb': 'Pvalb'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '4celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'e', 'e4Rorb': 'e', 'e4other': 'e', \n",
    "                         'e4Nr5a1': 'e', 'i6Htr3a': 'Htr3a', 'i6Sst': 'Sst', 'e6Ntsr1': 'e', 'i23Pvalb': 'Pvalb', 'i23Htr3a': 'Htr3a',\n",
    "                         'i1Htr3a': 'Htr3a', 'i4Sst': 'Sst', 'e5Rbp4': 'e', 'e5noRbp4': 'e', 'i23Sst': 'Sst', 'i4Htr3a': 'Htr3a',\n",
    "                         'i6Pvalb': 'Pvalb', 'i5Pvalb': 'Pvalb', 'i4Pvalb': 'Pvalb'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '5layers':\n",
    "                aggr_dict = {'e23Cux2': '23', 'i5Sst': '5', 'i5Htr3a': '5', 'e4Scnn1a': '4', 'e4Rorb': '4', 'e4other': '4',\n",
    "                         'e4Nr5a1': '4', 'i6Htr3a': '6', 'i6Sst': '6', 'e6Ntsr1': '6', 'i23Pvalb': '23', 'i23Htr3a': '23',\n",
    "                         'i1Htr3a': '1', 'i4Sst': '4', 'e5Rbp4': '5', 'e5noRbp4': '5', 'i23Sst': '23', 'i4Htr3a': '4',\n",
    "                         'i6Pvalb': '6', 'i5Pvalb': '5', 'i4Pvalb': '4'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '2celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'i', 'i5Htr3a': 'i', 'e4Scnn1a': 'e', 'e4Rorb': 'e',\n",
    "                         'e4other': 'e', 'e4Nr5a1': 'e', 'i6Htr3a': 'i', 'i6Sst': 'i', 'e6Ntsr1': 'e',\n",
    "                         'i23Pvalb': 'i', 'i23Htr3a': 'i', 'i1Htr3a': 'i', 'i4Sst': 'i', 'e5Rbp4': 'e',\n",
    "                         'e5noRbp4': 'e', 'i23Sst': 'i', 'i4Htr3a': 'i', 'i6Pvalb': 'i', 'i5Pvalb': 'i',\n",
    "                         'i4Pvalb': 'i'}    \n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "\n",
    "\n",
    "    # Split into train/val/test sets\n",
    "    dataset.split_cell_train_val_test(test_size=test_size, val_size=val_size, seed=cell_split_seed)\n",
    "    #dataset.split_trial_train_val_test(test_size=0.2, val_size=0.2, temp=True, seed=1234)\n",
    "\n",
    "    # bining\n",
    "    dataset.set_bining_parameters(bin_size=bin_size) # in seconds, so this is 200ms\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data and prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler():\n",
    "    if distribution == 'ISIFR':\n",
    "        fr_train_scaler, fr_bins = get_train_scaler(dataset,sampler=sampler,transform='firing_rate',bins=fr_dist_bins)\n",
    "        isi_train_scaler, isi_bins = get_train_scaler(dataset,sampler=sampler,transform='interspike_interval',bins=isi_dist_bins)\n",
    "        train_scaler = [isi_train_scaler,fr_train_scaler]\n",
    "        train_dataset = ISIFRDistributionDataset(dataset, isi_bins=isi_bins,fr_bins=fr_bins, isi_scaler=isi_train_scaler, fr_scaler=fr_train_scaler, mode='train', sampler=sampler)\n",
    "        # fix population for validation set and test set (they will be different of course)\n",
    "        val_dataset = ISIFRDistributionDataset(dataset, isi_bins=isi_bins,fr_bins=fr_bins, isi_scaler=isi_train_scaler, fr_scaler=fr_train_scaler, mode='val', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "        test_dataset = ISIFRDistributionDataset(dataset, isi_bins=isi_bins,fr_bins=fr_bins, isi_scaler=isi_train_scaler, fr_scaler=fr_train_scaler, mode='test', sampler=sampler,cell_random_seed=cell_sample_seed)    \n",
    "\n",
    "    \n",
    "    elif 'ISI' in distribution:\n",
    "        if distribution == 'log_ISI':\n",
    "            train_scaler, bins = get_train_scaler(dataset,sampler=sampler,transform='log_interspike_interval',bins=isi_dist_bins)        \n",
    "        else:\n",
    "            train_scaler, bins = get_train_scaler(dataset,sampler=sampler,transform='interspike_interval',bins=isi_dist_bins)\n",
    "        # Create Pytorch datasets\n",
    "        train_dataset = ISIDistributionDataset(dataset, min_isi=0, max_isi=0.4, bins=isi_dist_bins, scaler=train_scaler, mode='train', sampler=sampler)\n",
    "        # fix population for validation set and test set (they will be different of course)\n",
    "        val_dataset = ISIDistributionDataset(dataset, min_isi=0, max_isi=0.4, bins=isi_dist_bins, scaler=train_scaler, mode='val', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "        test_dataset = ISIDistributionDataset(dataset, min_isi=0, max_isi=0.4, bins=isi_dist_bins, scaler=train_scaler, mode='test', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "\n",
    "    elif distribution == 'FR':\n",
    "        train_scaler, bins = get_train_scaler(dataset,sampler=sampler,transform='firing_rate',bins=fr_dist_bins)\n",
    "        train_dataset = FRDistributionDataset(dataset, bins=fr_dist_bins, scaler=train_scaler, mode='train', sampler=sampler)\n",
    "        # fix population for validation set and test set (they will be different of course)\n",
    "        val_dataset = FRDistributionDataset(dataset, bins=fr_dist_bins, scaler=train_scaler, mode='val', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "        test_dataset = FRDistributionDataset(dataset, bins=fr_dist_bins, scaler=train_scaler, mode='test', sampler=sampler,cell_random_seed=cell_sample_seed)    \n",
    "\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=train_dataset.collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=val_dataset.collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=test_dataset.collate_fn)\n",
    "    return train_scaler, train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def get_train_scaler(dataset,sampler='R20',transform='interspike_interval',cell_random_seed=1,bins=list(np.arange(0,0.402,0.002)),scaler_type='Z'):\n",
    "    trials = dataset.get_trials('train')\n",
    "    train_mask = dataset._cell_split['train']\n",
    "    X_bank = []\n",
    "    for trial in trials:\n",
    "        X, y, m = dataset.sample(mode='train',trial_id=trial,sampler=sampler,transform=transform,preselected_mask=train_mask)\n",
    "        for x in X:\n",
    "            X_bank.append(x)\n",
    "\n",
    "    print(min([np.min(x) for x in X_bank if len(x)>0]),max([np.max(x) for x in X_bank if len(x)>0]))\n",
    "            \n",
    "    if type(bins) == int:\n",
    "        ser,adaptive_bins = pd.qcut(np.ndarray.flatten(np.hstack(X_bank)),bins,retbins=True)\n",
    "        xi_hists = []\n",
    "        for xi in X_bank:\n",
    "            if len(xi) > 0:\n",
    "                xi_hist = np.histogram(xi,bins=adaptive_bins)[0]\n",
    "            else:\n",
    "                xi_hist = np.zeros(bins)\n",
    "            xi_hists.append(xi_hist)\n",
    "        xi_hists_array = np.vstack(xi_hists)\n",
    "        if scaler_type == 'Z':\n",
    "            train_scaler = StandardScaler()\n",
    "            train_scaler = train_scaler.fit(xi_hists_array)\n",
    "        elif scaler_type == \"MinMax\":\n",
    "            train_scaler = MinMaxScaler()\n",
    "            train_scaler = train_scaler.fit(xi_hists_array)\n",
    "        bins = adaptive_bins\n",
    "        \n",
    "    elif (type(bins) == list) | (type(bins) == tuple):\n",
    "        xi_hists = []\n",
    "        for xi in X_bank:\n",
    "            xi_hist = np.histogram(xi,bins=bins)[0]\n",
    "            xi_hists.append(xi_hist)\n",
    "        xi_hists_array = np.vstack(xi_hists)        \n",
    "        if scaler_type == 'Z':\n",
    "            train_scaler = StandardScaler()\n",
    "            train_scaler = train_scaler.fit(xi_hists_array)\n",
    "        elif scaler_type == \"MinMax\":\n",
    "            train_scaler = MinMaxScaler()\n",
    "            train_scaler = train_scaler.fit(xi_hists_array)\n",
    "    \n",
    "    return train_scaler, bins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=None):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        '''\n",
    "        if log_interval and batch_idx % log_interval == 0:\n",
    "            \n",
    "            print('Train Epoch: {} [({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, 100. * batch_idx / len(train_loader), loss.item()))\n",
    "                '''\n",
    "\n",
    "def test(model, device, loader, tag, labels=dataset.cell_type_labels):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds = []\n",
    "    corrects = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            pred_ = np.ndarray.flatten(pred.cpu().numpy())\n",
    "            targ_ = target.cpu().numpy()\n",
    "            preds.append(pred_)\n",
    "            corrects.append(targ_)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += len(target)\n",
    "    '''\n",
    "    print('{} set: Accuracy: {}/{} ({:.0f}%)'.format(tag,\n",
    "        correct, total,\n",
    "        100. * correct / total))\n",
    "        '''\n",
    "    corrects = np.hstack(corrects)\n",
    "    preds = np.hstack(preds)\n",
    "    acc = f1_score(corrects,preds,average='macro')\n",
    "    print('F1:',acc)\n",
    "    cm = confusion_matrix(corrects,preds,normalize='true')\n",
    "    return cm,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(cm,cell_type_labels=dataset.cell_type_labels,outfilename='ex_cm.png'):\n",
    "    plt.figure(figsize = (10,7))\n",
    "    plt.set_cmap('Reds')\n",
    "    plt.xticks(range(len(cell_type_labels)),dataset.cell_type_labels,rotation='vertical')\n",
    "    plt.yticks(range(len(cell_type_labels)),dataset.cell_type_labels)\n",
    "    plt.imshow(cm,vmin=0,vmax=1)\n",
    "    plt.colorbar()\n",
    "    for (j,i),label in np.ndenumerate(cm):\n",
    "        if i == j:\n",
    "            plt.text(i,j,np.round(label,2),ha='center',va='center',fontsize='small',weight='bold')        \n",
    "        else:\n",
    "            plt.text(i,j,np.round(label,2),ha='center',va='center',fontsize='small')\n",
    "    if outfilename == None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(outfilename,dpi=300)    \n",
    "    plt.clf()\n",
    "\n",
    "def plot_accs(train_accs,val_accs,test_accs,outfilename='ex_acc.png'):\n",
    "    plt.plot(train_accs,label='Train')\n",
    "    plt.plot(val_accs,label='Validate')\n",
    "    plt.plot(test_accs,label='Test')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('F-Measure')\n",
    "    plt.legend()\n",
    "    if outfilename == None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(outfilename,dpi=300)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['17celltypes', 'ISI', 1234, 0.2, 0, 0.401, 5e-05, 0, 51, 1]\n",
      "Found processed pickle. Loading from '../data/processed/dataset.pkl'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-07a0d4e49161>:17: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if dist_params!=prev_dist_params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load\n",
      "Scale\n",
      "0.0026557706966627848 98.05970326641108\n",
      "F1: 0.18916435244273475\n",
      "F1: 0.1383919154007944\n",
      "F1: 0.14799819093827582\n",
      "\n",
      "\n",
      "F1: 0.4142392985710413\n",
      "F1: 0.19260943685161394\n",
      "F1: 0.19113063260093238\n",
      "\n",
      "\n",
      "F1: 0.5305468984718305\n",
      "F1: 0.20806080167133334\n",
      "F1: 0.21222997146052136\n",
      "\n",
      "\n",
      "F1: 0.5597980702576882\n",
      "F1: 0.2020895225590915\n",
      "F1: 0.22420991761557238\n",
      "\n",
      "\n",
      "F1: 0.58625074661062\n",
      "F1: 0.19691447533866102\n",
      "F1: 0.2177106827328939\n",
      "\n",
      "\n",
      "F1: 0.6141036691802909\n",
      "F1: 0.21942143302719852\n",
      "F1: 0.21709806705252083\n",
      "\n",
      "\n",
      "F1: 0.6249156094497158\n",
      "F1: 0.21635442862093854\n",
      "F1: 0.2349450050464663\n",
      "\n",
      "\n",
      "F1: 0.673675100446019\n",
      "F1: 0.21388979325248877\n",
      "F1: 0.2202373587461721\n",
      "\n",
      "\n",
      "F1: 0.6720930288286349\n",
      "F1: 0.20064280293782066\n",
      "F1: 0.20987270700062888\n",
      "\n",
      "\n",
      "F1: 0.693258013859217\n",
      "F1: 0.19561967010112108\n",
      "F1: 0.22445018977505282\n",
      "\n",
      "\n",
      "F1: 0.6895093922781131\n",
      "F1: 0.19775593539905198\n",
      "F1: 0.21584603722934326\n",
      "\n",
      "\n",
      "F1: 0.7079520098375183\n",
      "F1: 0.20074150455838422\n",
      "F1: 0.22573672316647025\n",
      "\n",
      "\n",
      "F1: 0.7102082236681654\n",
      "F1: 0.2102418706682043\n",
      "F1: 0.22390881947345276\n",
      "\n",
      "\n",
      "F1: 0.6938768197643728\n",
      "F1: 0.2049700180610179\n",
      "F1: 0.2089939812795809\n",
      "\n",
      "\n",
      "F1: 0.7255420871013648\n",
      "F1: 0.2137387060277093\n",
      "F1: 0.2350503686027006\n",
      "\n",
      "\n",
      "F1: 0.7267577667331734\n",
      "F1: 0.21035139509032022\n",
      "F1: 0.22026612652140662\n",
      "\n",
      "\n",
      "F1: 0.7392119387427056\n",
      "F1: 0.22444921340831653\n",
      "F1: 0.21898361088404475\n",
      "\n",
      "\n",
      "F1: 0.7427187113534373\n",
      "F1: 0.23138284794263267\n",
      "F1: 0.22973662585294866\n",
      "\n",
      "\n",
      "F1: 0.7489104339787074\n",
      "F1: 0.21525276619860972\n",
      "F1: 0.2228182748260068\n",
      "\n",
      "\n",
      "F1: 0.7532383989791837\n",
      "F1: 0.22315268030625035\n",
      "F1: 0.22505471611767977\n",
      "\n",
      "\n",
      "F1: 0.7512765551420597\n",
      "F1: 0.21801784199336535\n",
      "F1: 0.2197097472545573\n",
      "\n",
      "\n",
      "F1: 0.746952379176768\n",
      "F1: 0.20917188339494627\n",
      "F1: 0.22152078525086272\n",
      "\n",
      "\n",
      "F1: 0.7574344814740023\n",
      "F1: 0.22066385833528573\n",
      "F1: 0.23519389904653354\n",
      "\n",
      "\n",
      "F1: 0.7471034369291156\n",
      "F1: 0.23506919200823365\n",
      "F1: 0.23463966702812086\n",
      "\n",
      "\n",
      "F1: 0.7697763495106487\n",
      "F1: 0.21089991863042826\n",
      "F1: 0.2235837835472339\n",
      "\n",
      "\n",
      "F1: 0.7690549738605787\n",
      "F1: 0.21534014525453546\n",
      "F1: 0.22383193173588917\n",
      "\n",
      "\n",
      "F1: 0.7585449177794718\n",
      "F1: 0.2213867686026383\n",
      "F1: 0.24700103734674658\n",
      "\n",
      "\n",
      "F1: 0.7626650566710919\n",
      "F1: 0.2228608996330792\n",
      "F1: 0.2418311245377571\n",
      "\n",
      "\n",
      "F1: 0.7660469121988807\n",
      "F1: 0.21827021906095873\n",
      "F1: 0.23268207427400023\n",
      "\n",
      "\n",
      "F1: 0.7665163862377689\n",
      "F1: 0.20757895530252812\n",
      "F1: 0.22343326159566806\n",
      "\n",
      "\n",
      "F1: 0.7655930389536448\n",
      "F1: 0.2099433652958868\n",
      "F1: 0.22270849493532802\n",
      "\n",
      "\n",
      "F1: 0.7682698167193467\n",
      "F1: 0.23105659063008221\n",
      "F1: 0.2342266143329426\n",
      "\n",
      "\n",
      "F1: 0.7654813951187949\n",
      "F1: 0.21921903848134197\n",
      "F1: 0.22064955040182005\n",
      "\n",
      "\n",
      "F1: 0.775078615868025\n",
      "F1: 0.21516662007491194\n",
      "F1: 0.224132343776887\n",
      "\n",
      "\n",
      "F1: 0.7734923313027883\n",
      "F1: 0.23020204258900262\n",
      "F1: 0.2316819209348121\n",
      "\n",
      "\n",
      "F1: 0.7816147936227198\n",
      "F1: 0.21647980093974709\n",
      "F1: 0.22088904525208578\n",
      "\n",
      "\n",
      "F1: 0.786321377346427\n",
      "F1: 0.22313203948961546\n",
      "F1: 0.22096104217041046\n",
      "\n",
      "\n",
      "F1: 0.7781726888301183\n",
      "F1: 0.22110245467279238\n",
      "F1: 0.22787240450684923\n",
      "\n",
      "\n",
      "F1: 0.7817603378666356\n",
      "F1: 0.22653940684726148\n",
      "F1: 0.229134157593871\n",
      "\n",
      "\n",
      "F1: 0.7882644657516595\n",
      "F1: 0.21753698965078916\n",
      "F1: 0.22485926775076093\n",
      "\n",
      "\n",
      "F1: 0.7699057551256006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ndim'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-07a0d4e49161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtrain_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mval_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mtest_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-fc2c2c6550f3>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, loader, tag, labels)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcorrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aschneider61/cell_type/src/pytorch_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mtrial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0misi_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_isi_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_isi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_isi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mnormed_isi_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misi_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0misi_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormed_isi_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aschneider61/cell_type/src/isi.py\u001b[0m in \u001b[0;36mcompute_isi_dist\u001b[0;34m(x, bins, min_isi, max_isi)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mx_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mx_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/lib/histograms.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ravel_and_check_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m     \u001b[0mbin_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniform_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_bin_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;31m# Histogram is an integer or a float array depending on the weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/lib/histograms.py\u001b[0m in \u001b[0;36m_get_bin_edges\u001b[0;34m(a, bins, range, weights)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mfirst_edge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_edge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_outer_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0mbin_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_edges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbin_edges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mndim\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3019\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3021\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hp_df = pd.read_csv('hp_grid_p6.csv') #this is a hyperparameter grid\n",
    "prev_dist_params = np.asarray([0]) #just empty initialization\n",
    "\n",
    "#FOR EVERY HYPERPARAMETER COMBINATION\n",
    "for index,row in hp_df.iterrows(): #just skipping the first row because I ran previously\n",
    "    \n",
    "    #LOAD THE HYPERPARAMETERS\n",
    "    hpc = row['hp_idx']\n",
    "    k,distribution,cell_split_seed,bin_size = row['k'],row['distribution'],row['cell_split_seed'],row['bin_size']\n",
    "    isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step,fr_dist_bins_start,fr_dist_bins_stop,fr_dist_bins_step = [row['isi_dist_bins_start'],row['isi_dist_bins_stop'],row['isi_dist_bins_step'],row['fr_dist_bins_start'],row['fr_dist_bins_stop'],row['fr_dist_bins_step']]\n",
    "    lr,n_hiddens = row['lr'],row['n_hiddens']\n",
    "    n_hiddens = [int(nh) for nh in n_hiddens.rsplit(',')]\n",
    "    dist_params = [k,distribution,cell_split_seed,bin_size,isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step,fr_dist_bins_start,fr_dist_bins_stop,fr_dist_bins_step,]\n",
    "    isi_dist_bins = list(np.arange(isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step))\n",
    "    fr_dist_bins = list(range(int(fr_dist_bins_start),int(fr_dist_bins_stop),int(fr_dist_bins_step)))\n",
    "    \n",
    "    #SET SOME HARDCODED HYPERPARAMETERS\n",
    "    batch_size = 1\n",
    "    n_class = int(''.join(filter(str.isdigit, k)))\n",
    "    cell_sample_seed = 1\n",
    "    test_size, val_size = 0.4,0.4 #should leave a training set of only 20%\n",
    "    \n",
    "    dropout_p = 0 #currently overfitting\n",
    "    epochs=50 #seems to be enough to converge    \n",
    "    \n",
    "    #IF THIS HYPERPARAMETER COMBO HAS DIFFERENT DATA (FEATURES OR CLASS LABELS) RELOAD AND RENORMALIZE THE DATA\n",
    "    if dist_params!=prev_dist_params: \n",
    "        dataset = Dataset('../data', force_process=False)\n",
    "        dataset.data_source = 'v1'\n",
    "        dataset.labels_col = 'pop_name'\n",
    "        sampler = 'R20'\n",
    "        dataset.num_trials_in_window = 33\n",
    "        print('Load')\n",
    "        dataset = load_data()\n",
    "        print('Scale')\n",
    "        train_scaler, train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = get_scaler()\n",
    "    \n",
    "    #RUN THE MODEL\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MLP(input_dims=train_dataset.num_bins, n_hiddens=n_hiddens, n_class=n_class, dropout_p=dropout_p).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_accs, val_accs, test_accs = [],[],[] #used to save the f1s across iterations\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval=5)\n",
    "        train_cm, train_acc = test(model, device, train_loader, 'Train')\n",
    "        val_cm, val_acc = test(model, device, val_loader, 'Val')\n",
    "        test_cm, test_acc = test(model, device, test_loader, 'Test')\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        test_accs.append(test_acc)\n",
    "    \n",
    "    #SAVE THE F1 SCORES ACROSS ITERATIONS AND FINAL CONFUSION MATRIX\n",
    "    accs_df = pd.DataFrame(list(zip(train_accs, val_accs, test_accs)),columns=['train_acc','test_acc','val_acc'])\n",
    "    accs_df.to_csv('{}_hp{}_f1s.csv'.format(distribution,str(hpc)),index=False)\n",
    "    np.save('{}_hp{}_cms.npy'.format(distribution,str(hpc)),np.dstack([train_cm,val_cm,test_cm]))\n",
    "    prev_dist_params = dist_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
