{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src import Dataset, ISIDistributionDataset, EdgeDistributionDataset, FRDistributionDataset, ISIFRDistributionDataset\n",
    "from src.network import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'i', 'i5Htr3a': 'i', 'e4Scnn1a': 'e', 'e4Rorb': 'e',\n",
    "         'e4other': 'e', 'e4Nr5a1': 'e', 'i6Htr3a': 'i', 'i6Sst': 'i', 'e6Ntsr1': 'e',\n",
    "         'i23Pvalb': 'i', 'i23Htr3a': 'i', 'i1Htr3a': 'i', 'i4Sst': 'i', 'e5Rbp4': 'e',\n",
    "         'e5noRbp4': 'e', 'i23Sst': 'i', 'i4Htr3a': 'i', 'i6Pvalb': 'i', 'i5Pvalb': 'i',\n",
    "         'i4Pvalb': 'i'}        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    if dataset.data_source == 'v1':\n",
    "        if dataset.labels_col == 'pop_name':\n",
    "            dataset.drop_dead_cells(cutoff=30)\n",
    "            keepers = ['e5Rbp4', 'e23Cux2', 'i6Pvalb', 'e4Scnn1a', 'i23Pvalb', 'i23Htr3a',\n",
    "             'e4Rorb', 'e4other', 'i5Pvalb', 'i4Pvalb', 'i23Sst', 'i4Sst', 'e4Nr5a1',\n",
    "             'i1Htr3a', 'e5noRbp4', 'i6Sst', 'e6Ntsr1']\n",
    "            dataset.drop_other_classes(classes_to_keep=keepers)\n",
    "            if k == '17celltypes':\n",
    "                pass #all filtering done above\n",
    "            elif k == '13celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e23', 'i5Sst': 'i5Sst', 'i5Htr3a': 'i5Htr3a', 'e4Scnn1a': 'e4', 'e4Rorb': 'e4',\n",
    "                         'e4other': 'e4', 'e4Nr5a1': 'e4', 'i6Htr3a': 'i6Htr3a', 'i6Sst': 'i6Sst', 'e6Ntsr1': 'e6',\n",
    "                         'i23Pvalb': 'i23Pvalb', 'i23Htr3a': 'i23Htr3a', 'i1Htr3a': 'i1Htr3a', 'i4Sst': 'i4Sst', 'e5Rbp4': 'e5',\n",
    "                         'e5noRbp4': 'e5', 'i23Sst': 'i23Sst', 'i4Htr3a': 'i4Htr3a', 'i6Pvalb': 'i6Pvalb', 'i5Pvalb': 'i5Pvalb',\n",
    "                         'i4Pvalb': 'i4Pvalb'}        \n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '11celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'Cux2', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'Scnn1a', 'e4Rorb': 'Rorb',\n",
    "                         'e4other': 'other', 'e4Nr5a1': 'Nr5a1', 'i6Htr3a': 'Htr3a', 'i6Sst': 'Sst', 'e6Ntsr1': 'Ntsr1',\n",
    "                         'i23Pvalb': 'Pvalb', 'i23Htr3a': 'Htr3a', 'i1Htr3a': 'Htr3a', 'i4Sst': 'Sst', 'e5Rbp4': 'Rbp4',\n",
    "                         'e5noRbp4': 'noRbp4', 'i23Sst': 'Sst', 'i4Htr3a': 'Htr3a', 'i6Pvalb': 'Pvalb', 'i5Pvalb': 'Pvalb',\n",
    "                         'i4Pvalb': 'Pvalb'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '4celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'e', 'e4Rorb': 'e', 'e4other': 'e', \n",
    "                         'e4Nr5a1': 'e', 'i6Htr3a': 'Htr3a', 'i6Sst': 'Sst', 'e6Ntsr1': 'e', 'i23Pvalb': 'Pvalb', 'i23Htr3a': 'Htr3a',\n",
    "                         'i1Htr3a': 'Htr3a', 'i4Sst': 'Sst', 'e5Rbp4': 'e', 'e5noRbp4': 'e', 'i23Sst': 'Sst', 'i4Htr3a': 'Htr3a',\n",
    "                         'i6Pvalb': 'Pvalb', 'i5Pvalb': 'Pvalb', 'i4Pvalb': 'Pvalb'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '5layers':\n",
    "                aggr_dict = {'e23Cux2': '23', 'i5Sst': '5', 'i5Htr3a': '5', 'e4Scnn1a': '4', 'e4Rorb': '4', 'e4other': '4',\n",
    "                         'e4Nr5a1': '4', 'i6Htr3a': '6', 'i6Sst': '6', 'e6Ntsr1': '6', 'i23Pvalb': '23', 'i23Htr3a': '23',\n",
    "                         'i1Htr3a': '1', 'i4Sst': '4', 'e5Rbp4': '5', 'e5noRbp4': '5', 'i23Sst': '23', 'i4Htr3a': '4',\n",
    "                         'i6Pvalb': '6', 'i5Pvalb': '5', 'i4Pvalb': '4'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '2celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'i', 'i5Htr3a': 'i', 'e4Scnn1a': 'e', 'e4Rorb': 'e',\n",
    "                         'e4other': 'e', 'e4Nr5a1': 'e', 'i6Htr3a': 'i', 'i6Sst': 'i', 'e6Ntsr1': 'e',\n",
    "                         'i23Pvalb': 'i', 'i23Htr3a': 'i', 'i1Htr3a': 'i', 'i4Sst': 'i', 'e5Rbp4': 'e',\n",
    "                         'e5noRbp4': 'e', 'i23Sst': 'i', 'i4Htr3a': 'i', 'i6Pvalb': 'i', 'i5Pvalb': 'i',\n",
    "                         'i4Pvalb': 'i'}    \n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "\n",
    "\n",
    "    # Split into train/val/test sets\n",
    "    dataset.split_cell_train_val_test(test_size=test_size, val_size=val_size, seed=cell_split_seed)\n",
    "    #dataset.split_trial_train_val_test(test_size=0.2, val_size=0.2, temp=True, seed=1234)\n",
    "\n",
    "    # bining\n",
    "    dataset.set_bining_parameters(bin_size=bin_size) # in seconds, so this is 200ms\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_scaler(dataset,sampler='R20',transform='interspike_interval',cell_random_seed=1,bins=list(np.arange(0,0.402,0.002))):\n",
    "    trials = dataset.get_trials('train')\n",
    "    X_bank = []\n",
    "    for trial in trials:\n",
    "        X, y, m = dataset.sample(mode='train',trial_id=trial,sampler=sampler,transform=transform,cell_random_seed=cell_random_seed)\n",
    "        for x in X:\n",
    "            X_bank.append(x)\n",
    "\n",
    "    if type(bins) == int:\n",
    "        ser,adaptive_bins = pd.qcut(np.ndarray.flatten(np.hstack(X_bank)),bins,retbins=True)\n",
    "        xi_hists = []\n",
    "        for xi in X_bank:\n",
    "            if len(xi) > 0:\n",
    "                xi_hist = np.histogram(xi,bins=adaptive_bins)[0]\n",
    "            else:\n",
    "                xi_hist = np.zeros(bins)\n",
    "            xi_hists.append(xi_hist)\n",
    "        xi_hists_array = np.vstack(xi_hists)        \n",
    "        train_scaler = StandardScaler()\n",
    "        train_scaler = train_scaler.fit(xi_hists_array)\n",
    "        bins = adaptive_bins\n",
    "        \n",
    "    elif (type(bins) == list) | (type(bins) == tuple):\n",
    "        xi_hists = []\n",
    "        for xi in X_bank:\n",
    "            xi_hist = np.histogram(xi,bins=bins)[0]\n",
    "            xi_hists.append(xi_hist)\n",
    "        xi_hists_array = np.vstack(xi_hists)        \n",
    "        train_scaler = StandardScaler()\n",
    "        train_scaler = train_scaler.fit(xi_hists_array)\n",
    "    \n",
    "    return train_scaler, bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found processed pickle. Loading from '../data/processed/dataset.pkl'.\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('../data', force_process=False)\n",
    "dataset.data_source = 'v1'\n",
    "dataset.labels_col = 'pop_name'\n",
    "k = '17celltypes'\n",
    "distribution = 'ISIFR'\n",
    "sampler = 'R20'\n",
    "dataset.num_trials_in_window = 33\n",
    "batch_size = 1\n",
    "n_class = int(''.join(filter(str.isdigit, k)))\n",
    "cell_sample_seed = 1\n",
    "cell_split_seed = 1234\n",
    "#cell_split_seed = 2345\n",
    "#cell_split_seed = 3456\n",
    "#cell_split_seed = 4567\n",
    "#cell_split_seed = 5678\n",
    "\n",
    "test_size, val_size = 0.2,0.2\n",
    "bin_size = 0.2\n",
    "#isi_dist_bins = 800\n",
    "#isi_dist_bins = np.arange(-5,5.1,0.1).tolist()\n",
    "isi_dist_bins = np.arange(0,0.401,0.0005).tolist()\n",
    "fr_dist_bins = list(range(0,51,1))\n",
    "#fr_dist_bins = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler():\n",
    "    if distribution == 'ISIFR':\n",
    "        fr_train_scaler, fr_bins = get_train_scaler(dataset,sampler=sampler,transform='firing_rate',cell_random_seed=1,bins=list(range(0,51,1)))\n",
    "        isi_train_scaler, isi_bins = get_train_scaler(dataset,sampler=sampler,transform='interspike_interval',cell_random_seed=1,bins=list(np.arange(0,0.402,0.002)))\n",
    "        train_scaler = [isi_train_scaler,fr_train_scaler]\n",
    "        train_dataset = ISIFRDistributionDataset(dataset, isi_bins=isi_bins,fr_bins=fr_bins, isi_scaler=isi_train_scaler, fr_scaler=fr_train_scaler, mode='train', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "        # fix population for validation set and test set (they will be different of course)\n",
    "        val_dataset = ISIFRDistributionDataset(dataset, isi_bins=isi_bins,fr_bins=fr_bins, isi_scaler=isi_train_scaler, fr_scaler=fr_train_scaler, mode='val', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "        test_dataset = ISIFRDistributionDataset(dataset, isi_bins=isi_bins,fr_bins=fr_bins, isi_scaler=isi_train_scaler, fr_scaler=fr_train_scaler, mode='test', sampler=sampler,cell_random_seed=cell_sample_seed)    \n",
    "\n",
    "    \n",
    "    elif 'ISI' in distribution:\n",
    "        if distribution == 'log_ISI':\n",
    "            train_scaler, bins = get_train_scaler(dataset,sampler=sampler,transform='log_interspike_interval',cell_random_seed=1,bins=isi_dist_bins)        \n",
    "        else:\n",
    "            train_scaler, bins = get_train_scaler(dataset,sampler=sampler,transform='interspike_interval',cell_random_seed=1,bins=isi_dist_bins)\n",
    "        # Create Pytorch datasets\n",
    "        train_dataset = ISIDistributionDataset(dataset, min_isi=0, max_isi=0.4, bins=isi_dist_bins, scaler=train_scaler, mode='train', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "        # fix population for validation set and test set (they will be different of course)\n",
    "        val_dataset = ISIDistributionDataset(dataset, min_isi=0, max_isi=0.4, bins=isi_dist_bins, scaler=train_scaler, mode='val', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "        test_dataset = ISIDistributionDataset(dataset, min_isi=0, max_isi=0.4, bins=isi_dist_bins, scaler=train_scaler, mode='test', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "\n",
    "    elif distribution == 'FR':\n",
    "        train_scaler, bins = get_train_scaler(dataset,sampler=sampler,transform='firing_rate',cell_random_seed=1,bins=list(range(0,51,1)))\n",
    "        train_dataset = FRDistributionDataset(dataset, bins=fr_dist_bins, scaler=train_scaler, mode='train', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "        # fix population for validation set and test set (they will be different of course)\n",
    "        val_dataset = FRDistributionDataset(dataset, bins=fr_dist_bins, scaler=train_scaler, mode='val', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "        test_dataset = FRDistributionDataset(dataset, bins=fr_dist_bins, scaler=train_scaler, mode='test', sampler=sampler,cell_random_seed=cell_sample_seed)    \n",
    "\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=train_dataset.collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=val_dataset.collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=test_dataset.collate_fn)\n",
    "    return train_scaler, train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=None):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if log_interval and batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, 100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, loader, tag, labels=dataset.cell_type_labels):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds = []\n",
    "    corrects = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            pred_ = np.ndarray.flatten(pred.cpu().numpy())\n",
    "            targ_ = target.cpu().numpy()\n",
    "            preds.append(pred_)\n",
    "            corrects.append(targ_)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += len(target)\n",
    "\n",
    "    print('{} set: Accuracy: {}/{} ({:.0f}%)'.format(tag,\n",
    "        correct, total,\n",
    "        100. * correct / total))\n",
    "    corrects = np.hstack(corrects)\n",
    "    preds = np.hstack(preds)\n",
    "    acc = f1_score(corrects,preds,average='macro')\n",
    "    print(acc)\n",
    "    cm = confusion_matrix(corrects,preds,normalize='true')\n",
    "    return cm,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "n_hiddens=[150,75,38]\n",
    "#n_hiddens=[150,75,38,75,150]\n",
    "#n_hiddens=[200,300,400,300,200,100,50]\n",
    "#n_hiddens = [200,200,200,150,150,150,100,100,100,50,50,50]\n",
    "#n_hiddens=[200,100,50]\n",
    "dropout_p = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data()\n",
    "train_scaler, train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = get_scaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(0%)]\tLoss: 2.839304\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(5%)]\tLoss: 2.833656\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(10%)]\tLoss: 2.833475\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(15%)]\tLoss: 2.834438\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(20%)]\tLoss: 2.833176\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(25%)]\tLoss: 2.834172\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(30%)]\tLoss: 2.833751\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(35%)]\tLoss: 2.840872\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(40%)]\tLoss: 2.837617\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(45%)]\tLoss: 2.838911\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(50%)]\tLoss: 2.835625\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(55%)]\tLoss: 2.836574\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(60%)]\tLoss: 2.834391\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(65%)]\tLoss: 3.621750\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(70%)]\tLoss: 2.835842\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(75%)]\tLoss: 2.835002\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(80%)]\tLoss: 2.835724\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(85%)]\tLoss: 2.834194\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(90%)]\tLoss: 2.838346\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train Epoch: 1 [(95%)]\tLoss: 2.833780\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "(340, 200) (340, 50) 340\n",
      "Train set: Accuracy: 2004/34000 (6%)\n",
      "0.007072040744058046\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sample() got an unexpected keyword argument 'isi_bins'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-36718988b5d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mval_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtest_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f58e8e479c32>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, loader, tag, labels)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcorrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aschneider61/cell_type/src/pytorch_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mtrial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mfr_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_fr_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mnormed_fr_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aschneider61/cell_type/src/data.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sample() got an unexpected keyword argument 'isi_bins'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MLP(input_dims=train_dataset.num_bins, n_hiddens=n_hiddens, n_class=n_class, dropout_p=dropout_p).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "train_accs, val_accs, test_accs = [],[],[]\n",
    "epochs=200\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval=5)\n",
    "    train_cm, train_acc = test(model, device, train_loader, 'Train')\n",
    "    val_cm, val_acc = test(model, device, val_loader, 'Val')\n",
    "    test_cm, test_acc = test(model, device, test_loader, 'Test')\n",
    "    print('\\n')\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_cm(test_cm,cell_type_labels=dataset.cell_type_labels,outfilename='4bestfr20nh_adaptive_cm.png')\n",
    "plot_accs(train_accs,val_accs,test_accs,outfilename='4bestfr20nh_adaptive_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(cm,cell_type_labels=dataset.cell_type_labels,outfilename='ex_cm.png'):\n",
    "    plt.figure(figsize = (10,7))\n",
    "    plt.set_cmap('Reds')\n",
    "    plt.xticks(range(len(cell_type_labels)),dataset.cell_type_labels,rotation='vertical')\n",
    "    plt.yticks(range(len(cell_type_labels)),dataset.cell_type_labels)\n",
    "    plt.imshow(cm,vmin=0,vmax=1)\n",
    "    plt.colorbar()\n",
    "    for (j,i),label in np.ndenumerate(cm):\n",
    "        if i == j:\n",
    "            plt.text(i,j,np.round(label,2),ha='center',va='center',fontsize='small',weight='bold')        \n",
    "        else:\n",
    "            plt.text(i,j,np.round(label,2),ha='center',va='center',fontsize='small')\n",
    "    if outfilename == None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(outfilename,dpi=300)    \n",
    "    plt.clf()\n",
    "\n",
    "def plot_accs(train_accs,val_accs,test_accs,outfilename='ex_acc.png'):\n",
    "    plt.plot(train_accs,label='Train')\n",
    "    plt.plot(val_accs,label='Validate')\n",
    "    plt.plot(test_accs,label='Test')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('F-Measure')\n",
    "    plt.legend()\n",
    "    if outfilename == None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(outfilename,dpi=300)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (model): Sequential(\n",
      "    (fc1): Linear(in_features=200, out_features=20, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (drop1): Dropout(p=0.2, inplace=False)\n",
      "    (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (relu2): ReLU()\n",
      "    (drop2): Dropout(p=0.2, inplace=False)\n",
      "    (fc3): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (relu3): ReLU()\n",
      "    (drop3): Dropout(p=0.2, inplace=False)\n",
      "    (out): Linear(in_features=20, out_features=17, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b88431970b93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader2' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MLP(input_dims=train_dataset.num_bins, n_hiddens=[20, 20, 20], n_class=n_class, dropout_p=0.2).to(device)\n",
    "print(model)\n",
    "\n",
    "lr = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "epochs=100\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader2, optimizer, epoch, log_interval=5)\n",
    "    test(model, device, train_loader2, 'Train')\n",
    "    test(model, device, val_loader2, 'Val')\n",
    "    test(model, device, test_loader2, 'Test')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(isi_dist_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isi_dist_bins_list = [50,100,200,400]\n",
    "for isi_dist_bins in isi_dist_bins_list:\n",
    "    dataset = load_data()\n",
    "    train_scaler, train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = get_scaler()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = MLP(input_dims=train_dataset.num_bins, n_hiddens=n_hiddens, n_class=n_class, dropout_p=dropout_p).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_accs, val_accs, test_accs = [],[],[]\n",
    "    epochs=20\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval=5)\n",
    "        train_cm, train_acc = test(model, device, train_loader, 'Train')\n",
    "        val_cm, val_acc = test(model, device, val_loader, 'Val')\n",
    "        test_cm, test_acc = test(model, device, test_loader, 'Test')\n",
    "        print('\\n')\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        test_accs.append(test_acc)\n",
    "    plot_cm(test_cm,outfilename='testcm_adaptive{}.png'.format(str(isi_dist_bins)))\n",
    "    plot_cm(train_cm,outfilename='traincm_adaptive{}.png'.format(str(isi_dist_bins)))\n",
    "    plot_cm(val_cm,outfilename='valcm_adaptive{}.png'.format(str(isi_dist_bins)))\n",
    "    plot_accs(train_accs,val_accs,test_accs,outfilename='acc_adaptive{}.png'.format(str(isi_dist_bins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isi_dist_bins_list = [np.arange(0,0.402,0.002).tolist(),np.arange(0,0.402,0.004).tolist(),np.arange(0,0.402,0.008).tolist(),np.arange(0,0.401,0.001).tolist()]\n",
    "for isi_dist_bins in isi_dist_bins_list:\n",
    "    dataset = load_data()\n",
    "    train_scaler, train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = get_scaler()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = MLP(input_dims=train_dataset.num_bins, n_hiddens=n_hiddens, n_class=n_class, dropout_p=dropout_p).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_accs, val_accs, test_accs = [],[],[]\n",
    "    epochs=20\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval=5)\n",
    "        train_cm, train_acc = test(model, device, train_loader, 'Train')\n",
    "        val_cm, val_acc = test(model, device, val_loader, 'Val')\n",
    "        test_cm, test_acc = test(model, device, test_loader, 'Test')\n",
    "        print('\\n')\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        test_accs.append(test_acc)  \n",
    "    plot_cm(test_cm,outfilename='testcm_even{}.png'.format(str(len(isi_dist_bins)-1)))\n",
    "    plot_cm(train_cm,outfilename='traincm_even{}.png'.format(str(len(isi_dist_bins)-1)))\n",
    "    plot_cm(val_cm,outfilename='valcm_even{}.png'.format(str(len(isi_dist_bins)-1)))\n",
    "    plot_accs(train_accs,val_accs,test_accs,outfilename='acc_even{}.png'.format(str(len(isi_dist_bins)-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_list = ['R10','R20','R40','R60','R80','R100']\n",
    "isi_dist_bins = np.arange(0,0.402,0.002).tolist()\n",
    "for sampler in sampler_list:\n",
    "    dataset = load_data()\n",
    "    train_scaler, train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = get_scaler()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = MLP(input_dims=train_dataset.num_bins, n_hiddens=n_hiddens, n_class=n_class, dropout_p=dropout_p).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_accs, val_accs, test_accs = [],[],[]\n",
    "    epochs=20\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval=5)\n",
    "        train_cm, train_acc = test(model, device, train_loader, 'Train')\n",
    "        val_cm, val_acc = test(model, device, val_loader, 'Val')\n",
    "        test_cm, test_acc = test(model, device, test_loader, 'Test')\n",
    "        print('\\n')\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        test_accs.append(test_acc)\n",
    "    plot_cm(test_cm,outfilename='testcm_{}.png'.format(str(sampler)))\n",
    "    plot_cm(train_cm,outfilename='traincm_{}.png'.format(str(sampler)))\n",
    "    plot_cm(val_cm,outfilename='valcm_{}.png'.format(str(sampler)))\n",
    "    plot_accs(train_accs,val_accs,test_accs,outfilename='acc_{}.png'.format(str(sampler)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isi_dist_bins = np.arange(0,0.402,0.002).tolist()\n",
    "sampler = 'R40'\n",
    "n_hiddens_list = [[10,10,10],[20,20,20],[30,30,30],[40,40,40],[50,50,50]]\n",
    "for n_hiddens in n_hiddens_list:\n",
    "    dataset = load_data()\n",
    "    train_scaler, train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = get_scaler()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = MLP(input_dims=train_dataset.num_bins, n_hiddens=n_hiddens, n_class=n_class, dropout_p=dropout_p).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_accs, val_accs, test_accs = [],[],[]\n",
    "    epochs=20\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval=5)\n",
    "        train_cm, train_acc = test(model, device, train_loader, 'Train')\n",
    "        val_cm, val_acc = test(model, device, val_loader, 'Val')\n",
    "        test_cm, test_acc = test(model, device, test_loader, 'Test')\n",
    "        print('\\n')\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        test_accs.append(test_acc)\n",
    "    plot_cm(test_cm,outfilename='testcm_nh{}.png'.format(str(n_hiddens[0])))\n",
    "    plot_cm(train_cm,outfilename='traincm_nh{}.png'.format(str(n_hiddens[0])))\n",
    "    plot_cm(val_cm,outfilename='valcm_nh{}.png'.format(str(n_hiddens[0])))\n",
    "    plot_accs(train_accs,val_accs,test_accs,outfilename='acc_nh{}.png'.format(str(n_hiddens[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_list = ['R10','R20','R60','R80','R100','R40']\n",
    "isi_dist_bins = np.arange(0,0.402,0.002).tolist()\n",
    "sampler = 'R40'\n",
    "n_hiddens_list = [[20],[20,20],[20,20,20],[20,20,20,20]]\n",
    "for n_hiddens in n_hiddens_list:\n",
    "    dataset = load_data()\n",
    "    train_scaler, train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = get_scaler()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = MLP(input_dims=train_dataset.num_bins, n_hiddens=n_hiddens, n_class=n_class, dropout_p=dropout_p).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_accs, val_accs, test_accs = [],[],[]\n",
    "    epochs=20\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval=5)\n",
    "        train_cm, train_acc = test(model, device, train_loader, 'Train')\n",
    "        val_cm, val_acc = test(model, device, val_loader, 'Val')\n",
    "        test_cm, test_acc = test(model, device, test_loader, 'Test')\n",
    "        print('\\n')\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        test_accs.append(test_acc)\n",
    "    plot_cm(test_cm,outfilename='testcm_nl{}.png'.format(str(len(n_hiddens))))\n",
    "    plot_cm(train_cm,outfilename='traincm_nl{}.png'.format(str(len(n_hiddens))))\n",
    "    plot_cm(val_cm,outfilename='valcm_nl{}.png'.format(str(len(n_hiddens))))\n",
    "    plot_accs(train_accs,val_accs,test_accs,outfilename='acc_nl{}.png'.format(str(len(n_hiddens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
