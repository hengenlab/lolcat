{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src import Dataset, ISIDistributionDataset, EdgeDistributionDataset, FRDistributionDataset\n",
    "from src.network import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found processed pickle. Loading from '../data/processed/dataset.pkl'.\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('../data', force_process=False)\n",
    "dataset.data_source = 'v1'\n",
    "dataset.labels_col = 'pop_name'\n",
    "k = '17celltypes'\n",
    "distribution = 'ISI'\n",
    "sampler = 'R20'\n",
    "dataset.num_trials_in_window = 100\n",
    "batch_size = 1\n",
    "n_class = int(''.join(filter(str.isdigit, k)))\n",
    "cell_sample_seed = 1\n",
    "cell_split_seed = 1234\n",
    "test_size, val_size = 0.2,0.2\n",
    "bin_size = 0.2\n",
    "isi_dist_bins = list(np.arange(0,0.402,0.002))\n",
    "fr_dist_bins = list(range(0,51,1))\n",
    "\n",
    "if dataset.data_source == 'v1':\n",
    "    if dataset.labels_col == 'pop_name':\n",
    "        dataset.drop_dead_cells(cutoff=30)\n",
    "        keepers = ['e5Rbp4', 'e23Cux2', 'i6Pvalb', 'e4Scnn1a', 'i23Pvalb', 'i23Htr3a',\n",
    "         'e4Rorb', 'e4other', 'i5Pvalb', 'i4Pvalb', 'i23Sst', 'i4Sst', 'e4Nr5a1',\n",
    "         'i1Htr3a', 'e5noRbp4', 'i6Sst', 'e6Ntsr1']\n",
    "        dataset.drop_other_classes(classes_to_keep=keepers)\n",
    "        if k == '17celltypes':\n",
    "            pass #all filtering done above\n",
    "        elif k == '13celltypes':\n",
    "            aggr_dict = {'e23Cux2': 'e23', 'i5Sst': 'i5Sst', 'i5Htr3a': 'i5Htr3a', 'e4Scnn1a': 'e4', 'e4Rorb': 'e4',\n",
    "                     'e4other': 'e4', 'e4Nr5a1': 'e4', 'i6Htr3a': 'i6Htr3a', 'i6Sst': 'i6Sst', 'e6Ntsr1': 'e6',\n",
    "                     'i23Pvalb': 'i23Pvalb', 'i23Htr3a': 'i23Htr3a', 'i1Htr3a': 'i1Htr3a', 'i4Sst': 'i4Sst', 'e5Rbp4': 'e5',\n",
    "                     'e5noRbp4': 'e5', 'i23Sst': 'i23Sst', 'i4Htr3a': 'i4Htr3a', 'i6Pvalb': 'i6Pvalb', 'i5Pvalb': 'i5Pvalb',\n",
    "                     'i4Pvalb': 'i4Pvalb'}        \n",
    "            dataset.aggregate_cell_classes(aggr_dict)\n",
    "        elif k == '11celltypes':\n",
    "            aggr_dict = {'e23Cux2': 'Cux2', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'Scnn1a', 'e4Rorb': 'Rorb',\n",
    "                     'e4other': 'other', 'e4Nr5a1': 'Nr5a1', 'i6Htr3a': 'Htr3a', 'i6Sst': 'Sst', 'e6Ntsr1': 'Ntsr1',\n",
    "                     'i23Pvalb': 'Pvalb', 'i23Htr3a': 'Htr3a', 'i1Htr3a': 'Htr3a', 'i4Sst': 'Sst', 'e5Rbp4': 'Rbp4',\n",
    "                     'e5noRbp4': 'noRbp4', 'i23Sst': 'Sst', 'i4Htr3a': 'Htr3a', 'i6Pvalb': 'Pvalb', 'i5Pvalb': 'Pvalb',\n",
    "                     'i4Pvalb': 'Pvalb'}\n",
    "            dataset.aggregate_cell_classes(aggr_dict)\n",
    "        elif k == '4celltypes':\n",
    "            aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'e', 'e4Rorb': 'e', 'e4other': 'e', \n",
    "                     'e4Nr5a1': 'e', 'i6Htr3a': 'Htr3a', 'i6Sst': 'Sst', 'e6Ntsr1': 'e', 'i23Pvalb': 'Pvalb', 'i23Htr3a': 'Htr3a',\n",
    "                     'i1Htr3a': 'Htr3a', 'i4Sst': 'Sst', 'e5Rbp4': 'e', 'e5noRbp4': 'e', 'i23Sst': 'Sst', 'i4Htr3a': 'Htr3a',\n",
    "                     'i6Pvalb': 'Pvalb', 'i5Pvalb': 'Pvalb', 'i4Pvalb': 'Pvalb'}\n",
    "            dataset.aggregate_cell_classes(aggr_dict)\n",
    "        elif k == '5layers':\n",
    "            aggr_dict = {'e23Cux2': '23', 'i5Sst': '5', 'i5Htr3a': '5', 'e4Scnn1a': '4', 'e4Rorb': '4', 'e4other': '4',\n",
    "                     'e4Nr5a1': '4', 'i6Htr3a': '6', 'i6Sst': '6', 'e6Ntsr1': '6', 'i23Pvalb': '23', 'i23Htr3a': '23',\n",
    "                     'i1Htr3a': '1', 'i4Sst': '4', 'e5Rbp4': '5', 'e5noRbp4': '5', 'i23Sst': '23', 'i4Htr3a': '4',\n",
    "                     'i6Pvalb': '6', 'i5Pvalb': '5', 'i4Pvalb': '4'}\n",
    "            dataset.aggregate_cell_classes(aggr_dict)\n",
    "            raise NotImplementedError    \n",
    "      \n",
    "        \n",
    "# Split into train/val/test sets\n",
    "dataset.split_cell_train_val_test(test_size=test_size, val_size=val_size, seed=cell_split_seed)\n",
    "#dataset.split_trial_train_val_test(test_size=0.2, val_size=0.2, temp=True, seed=1234)\n",
    "\n",
    "# bining\n",
    "dataset.set_bining_parameters(bin_size=bin_size) # in seconds, so this is 200ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_scaler(dataset,sampler='R20',transform='interspike_interval',cell_random_seed=1,bins=list(np.arange(0,0.402,0.002))):\n",
    "    trials = dataset.get_trials('train')\n",
    "    X_bank = []\n",
    "    for trial in trials:\n",
    "        X, y, m = dataset.sample(mode='train',trial_id=trial,sampler=sampler,transform=transform,cell_random_seed=cell_random_seed)\n",
    "        for x in X:\n",
    "            X_bank.append(x)\n",
    "\n",
    "    if type(bins) == int:\n",
    "        ser,adaptive_bins = pd.qcut(np.ndarray.flatten(np.hstack(X_bank)),bins,retbins=True)\n",
    "        xi_hists = []\n",
    "        for xi in X_bank:\n",
    "            if len(xi) > 0:\n",
    "                xi_hist = np.histogram(xi,bins=adaptive_bins)[0]\n",
    "            else:\n",
    "                xi_hist = np.zeros(bins)\n",
    "            xi_hists.append(xi_hist)\n",
    "        xi_hists_array = np.vstack(xi_hists)        \n",
    "        train_scaler = StandardScaler()\n",
    "        train_scaler = train_scaler.fit(xi_hists_array)\n",
    "        bins = adaptive_bins\n",
    "        \n",
    "    elif (type(bins) == list) | (type(bins) == tuple):\n",
    "        xi_hists = []\n",
    "        for xi in X_bank:\n",
    "            xi_hist = np.histogram(xi,bins=bins)[0]\n",
    "            xi_hists.append(xi_hist)\n",
    "        xi_hists_array = np.vstack(xi_hists)        \n",
    "        train_scaler = StandardScaler()\n",
    "        train_scaler = train_scaler.fit(xi_hists_array)\n",
    "    \n",
    "    return train_scaler, bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if distribution == 'ISI':\n",
    "    train_scaler, bins = get_train_scaler(dataset,sampler=sampler,transform='interspike_interval',cell_random_seed=1,bins=list(np.arange(0,0.402,0.002)))\n",
    "    # Create Pytorch datasets\n",
    "    train_dataset = ISIDistributionDataset(dataset, min_isi=0, max_isi=0.4, bins=isi_dist_bins, scaler=train_scaler, mode='train', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "    # fix population for validation set and test set (they will be different of course)\n",
    "    val_dataset = ISIDistributionDataset(dataset, min_isi=0, max_isi=0.4, bins=isi_dist_bins, scaler=train_scaler, mode='val', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "    test_dataset = ISIDistributionDataset(dataset, min_isi=0, max_isi=0.4, bins=isi_dist_bins, scaler=train_scaler, mode='test', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "\n",
    "elif distribution == 'FR':\n",
    "    train_scaler, bins = get_train_scaler(dataset,sampler=sampler,transform='firing_rate',cell_random_seed=1,bins=list(range(0,51,1)))\n",
    "    train_dataset = FRDistributionDataset(dataset, bins=fr_dist_bins, scaler=train_scaler, mode='train', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "    # fix population for validation set and test set (they will be different of course)\n",
    "    val_dataset = FRDistributionDataset(dataset, bins=fr_dist_bins, scaler=train_scaler, mode='val', sampler=sampler,cell_random_seed=cell_sample_seed)\n",
    "    test_dataset = FRDistributionDataset(dataset, bins=fr_dist_bins, scaler=train_scaler, mode='test', sampler=sampler,cell_random_seed=cell_sample_seed)    \n",
    "    \n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=train_dataset.collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=val_dataset.collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=test_dataset.collate_fn)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=None):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if log_interval and batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, 100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, loader, tag):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += len(target)\n",
    "\n",
    "    print('{} set: Accuracy: {}/{} ({:.0f}%)'.format(tag,\n",
    "        correct, total,\n",
    "        100. * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [(0%)]\tLoss: 2.843848\n",
      "Train Epoch: 1 [(5%)]\tLoss: 2.754345\n",
      "Train Epoch: 1 [(10%)]\tLoss: 2.788282\n",
      "Train Epoch: 1 [(15%)]\tLoss: 2.646100\n",
      "Train Epoch: 1 [(20%)]\tLoss: 2.591560\n",
      "Train Epoch: 1 [(25%)]\tLoss: 2.748532\n",
      "Train Epoch: 1 [(30%)]\tLoss: 2.520421\n",
      "Train Epoch: 1 [(35%)]\tLoss: 2.569833\n",
      "Train Epoch: 1 [(40%)]\tLoss: 2.417063\n",
      "Train Epoch: 1 [(45%)]\tLoss: 2.385446\n",
      "Train Epoch: 1 [(50%)]\tLoss: 2.343421\n",
      "Train Epoch: 1 [(55%)]\tLoss: 2.247983\n",
      "Train Epoch: 1 [(60%)]\tLoss: 2.302302\n",
      "Train Epoch: 1 [(65%)]\tLoss: 2.226477\n",
      "Train Epoch: 1 [(70%)]\tLoss: 2.160133\n",
      "Train Epoch: 1 [(75%)]\tLoss: 2.195602\n",
      "Train Epoch: 1 [(80%)]\tLoss: 2.222525\n",
      "Train Epoch: 1 [(85%)]\tLoss: 2.216459\n",
      "Train Epoch: 1 [(90%)]\tLoss: 2.594265\n",
      "Train Epoch: 1 [(95%)]\tLoss: 2.114953\n",
      "Train set: Accuracy: 9986/34000 (29%)\n",
      "Val set: Accuracy: 8647/34000 (25%)\n",
      "Test set: Accuracy: 7028/34000 (21%)\n",
      "\n",
      "\n",
      "Train Epoch: 2 [(0%)]\tLoss: 2.213670\n",
      "Train Epoch: 2 [(5%)]\tLoss: 3.210582\n",
      "Train Epoch: 2 [(10%)]\tLoss: 2.596845\n",
      "Train Epoch: 2 [(15%)]\tLoss: 2.429913\n",
      "Train Epoch: 2 [(20%)]\tLoss: 2.123546\n",
      "Train Epoch: 2 [(25%)]\tLoss: 2.302381\n",
      "Train Epoch: 2 [(30%)]\tLoss: 2.014313\n",
      "Train Epoch: 2 [(35%)]\tLoss: 2.062371\n",
      "Train Epoch: 2 [(40%)]\tLoss: 2.544181\n",
      "Train Epoch: 2 [(45%)]\tLoss: 2.090374\n",
      "Train Epoch: 2 [(50%)]\tLoss: 2.167981\n",
      "Train Epoch: 2 [(55%)]\tLoss: 1.982961\n",
      "Train Epoch: 2 [(60%)]\tLoss: 2.065616\n",
      "Train Epoch: 2 [(65%)]\tLoss: 2.141060\n",
      "Train Epoch: 2 [(70%)]\tLoss: 2.002795\n",
      "Train Epoch: 2 [(75%)]\tLoss: 2.341078\n",
      "Train Epoch: 2 [(80%)]\tLoss: 1.995127\n",
      "Train Epoch: 2 [(85%)]\tLoss: 1.952248\n",
      "Train Epoch: 2 [(90%)]\tLoss: 1.997752\n",
      "Train Epoch: 2 [(95%)]\tLoss: 1.941147\n",
      "Train set: Accuracy: 12198/34000 (36%)\n",
      "Val set: Accuracy: 9322/34000 (27%)\n",
      "Test set: Accuracy: 7402/34000 (22%)\n",
      "\n",
      "\n",
      "Train Epoch: 3 [(0%)]\tLoss: 1.908960\n",
      "Train Epoch: 3 [(5%)]\tLoss: 1.988322\n",
      "Train Epoch: 3 [(10%)]\tLoss: 1.949692\n",
      "Train Epoch: 3 [(15%)]\tLoss: 2.225590\n",
      "Train Epoch: 3 [(20%)]\tLoss: 2.325628\n",
      "Train Epoch: 3 [(25%)]\tLoss: 1.926312\n",
      "Train Epoch: 3 [(30%)]\tLoss: 1.897805\n",
      "Train Epoch: 3 [(35%)]\tLoss: 1.843817\n",
      "Train Epoch: 3 [(40%)]\tLoss: 2.446866\n",
      "Train Epoch: 3 [(45%)]\tLoss: 1.899008\n",
      "Train Epoch: 3 [(50%)]\tLoss: 2.027329\n",
      "Train Epoch: 3 [(55%)]\tLoss: 1.835522\n",
      "Train Epoch: 3 [(60%)]\tLoss: 1.783021\n",
      "Train Epoch: 3 [(65%)]\tLoss: 1.849158\n",
      "Train Epoch: 3 [(70%)]\tLoss: 1.851039\n",
      "Train Epoch: 3 [(75%)]\tLoss: 1.873608\n",
      "Train Epoch: 3 [(80%)]\tLoss: 2.406860\n",
      "Train Epoch: 3 [(85%)]\tLoss: 1.936820\n",
      "Train Epoch: 3 [(90%)]\tLoss: 1.837117\n",
      "Train Epoch: 3 [(95%)]\tLoss: 1.808460\n",
      "Train set: Accuracy: 12648/34000 (37%)\n",
      "Val set: Accuracy: 8847/34000 (26%)\n",
      "Test set: Accuracy: 7059/34000 (21%)\n",
      "\n",
      "\n",
      "Train Epoch: 4 [(0%)]\tLoss: 1.895144\n",
      "Train Epoch: 4 [(5%)]\tLoss: 2.849602\n",
      "Train Epoch: 4 [(10%)]\tLoss: 2.036729\n",
      "Train Epoch: 4 [(15%)]\tLoss: 2.019526\n",
      "Train Epoch: 4 [(20%)]\tLoss: 3.099030\n",
      "Train Epoch: 4 [(25%)]\tLoss: 1.959709\n",
      "Train Epoch: 4 [(30%)]\tLoss: 1.978649\n",
      "Train Epoch: 4 [(35%)]\tLoss: 1.948190\n",
      "Train Epoch: 4 [(40%)]\tLoss: 1.888306\n",
      "Train Epoch: 4 [(45%)]\tLoss: 3.021754\n",
      "Train Epoch: 4 [(50%)]\tLoss: 2.046411\n",
      "Train Epoch: 4 [(55%)]\tLoss: 1.883201\n",
      "Train Epoch: 4 [(60%)]\tLoss: 1.758493\n",
      "Train Epoch: 4 [(65%)]\tLoss: 2.006866\n",
      "Train Epoch: 4 [(70%)]\tLoss: 1.815000\n",
      "Train Epoch: 4 [(75%)]\tLoss: 2.613397\n",
      "Train Epoch: 4 [(80%)]\tLoss: 1.787756\n",
      "Train Epoch: 4 [(85%)]\tLoss: 1.771554\n",
      "Train Epoch: 4 [(90%)]\tLoss: 1.956387\n",
      "Train Epoch: 4 [(95%)]\tLoss: 1.811084\n",
      "Train set: Accuracy: 14530/34000 (43%)\n",
      "Val set: Accuracy: 9628/34000 (28%)\n",
      "Test set: Accuracy: 7683/34000 (23%)\n",
      "\n",
      "\n",
      "Train Epoch: 5 [(0%)]\tLoss: 2.511065\n",
      "Train Epoch: 5 [(5%)]\tLoss: 2.244924\n",
      "Train Epoch: 5 [(10%)]\tLoss: 1.738325\n",
      "Train Epoch: 5 [(15%)]\tLoss: 2.416802\n",
      "Train Epoch: 5 [(20%)]\tLoss: 2.767611\n",
      "Train Epoch: 5 [(25%)]\tLoss: 1.796820\n",
      "Train Epoch: 5 [(30%)]\tLoss: 1.960969\n",
      "Train Epoch: 5 [(35%)]\tLoss: 2.176240\n",
      "Train Epoch: 5 [(40%)]\tLoss: 2.359988\n",
      "Train Epoch: 5 [(45%)]\tLoss: 1.791876\n",
      "Train Epoch: 5 [(50%)]\tLoss: 1.689425\n",
      "Train Epoch: 5 [(55%)]\tLoss: 1.685881\n",
      "Train Epoch: 5 [(60%)]\tLoss: 1.712728\n",
      "Train Epoch: 5 [(65%)]\tLoss: 2.018082\n",
      "Train Epoch: 5 [(70%)]\tLoss: 2.723712\n",
      "Train Epoch: 5 [(75%)]\tLoss: 1.772512\n",
      "Train Epoch: 5 [(80%)]\tLoss: 1.786635\n",
      "Train Epoch: 5 [(85%)]\tLoss: 1.729353\n",
      "Train Epoch: 5 [(90%)]\tLoss: 3.064773\n",
      "Train Epoch: 5 [(95%)]\tLoss: 1.768524\n",
      "Train set: Accuracy: 14231/34000 (42%)\n",
      "Val set: Accuracy: 9212/34000 (27%)\n",
      "Test set: Accuracy: 8453/34000 (25%)\n",
      "\n",
      "\n",
      "Train Epoch: 6 [(0%)]\tLoss: 2.035769\n",
      "Train Epoch: 6 [(5%)]\tLoss: 1.725036\n",
      "Train Epoch: 6 [(10%)]\tLoss: 1.762119\n",
      "Train Epoch: 6 [(15%)]\tLoss: 1.774533\n",
      "Train Epoch: 6 [(20%)]\tLoss: 2.613849\n",
      "Train Epoch: 6 [(25%)]\tLoss: 1.677450\n",
      "Train Epoch: 6 [(30%)]\tLoss: 2.024437\n",
      "Train Epoch: 6 [(35%)]\tLoss: 2.511156\n",
      "Train Epoch: 6 [(40%)]\tLoss: 1.924250\n",
      "Train Epoch: 6 [(45%)]\tLoss: 1.682835\n",
      "Train Epoch: 6 [(50%)]\tLoss: 1.778585\n",
      "Train Epoch: 6 [(55%)]\tLoss: 1.691117\n",
      "Train Epoch: 6 [(60%)]\tLoss: 1.751726\n",
      "Train Epoch: 6 [(65%)]\tLoss: 1.738967\n",
      "Train Epoch: 6 [(70%)]\tLoss: 2.181168\n",
      "Train Epoch: 6 [(75%)]\tLoss: 1.772565\n",
      "Train Epoch: 6 [(80%)]\tLoss: 1.744423\n",
      "Train Epoch: 6 [(85%)]\tLoss: 1.732582\n",
      "Train Epoch: 6 [(90%)]\tLoss: 1.985103\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MLP(input_dims=train_dataset.num_bins, n_hiddens=[20,20,20], n_class=n_class, dropout_p=0.2).to(device)\n",
    "\n",
    "lr = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "epochs=100\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval=5)\n",
    "    test(model, device, train_loader, 'Train')\n",
    "    test(model, device, val_loader, 'Val')\n",
    "    test(model, device, test_loader, 'Test')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MLP(input_dims=train_dataset.num_bins, n_hiddens=[20, 20, 20], n_class=n_class, dropout_p=0.2).to(device)\n",
    "print(model)\n",
    "\n",
    "lr = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "epochs=100\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader2, optimizer, epoch, log_interval=5)\n",
    "    test(model, device, train_loader2, 'Train')\n",
    "    test(model, device, val_loader2, 'Val')\n",
    "    test(model, device, test_loader2, 'Test')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
