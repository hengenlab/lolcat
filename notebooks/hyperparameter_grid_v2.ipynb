{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found processed pickle. Loading from '../data/processed/v1_dataset.pkl'.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "from src import Dataset, FiringRates, ISIDistribution\n",
    "from src.network import MLP\n",
    "from src.augmentations import slice_data\n",
    "\n",
    "aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'i', 'i5Htr3a': 'i', 'e4Scnn1a': 'e', 'e4Rorb': 'e',\n",
    "         'e4other': 'e', 'e4Nr5a1': 'e', 'i6Htr3a': 'i', 'i6Sst': 'i', 'e6Ntsr1': 'e',\n",
    "         'i23Pvalb': 'i', 'i23Htr3a': 'i', 'i1Htr3a': 'i', 'i4Sst': 'i', 'e5Rbp4': 'e',\n",
    "         'e5noRbp4': 'e', 'i23Sst': 'i', 'i4Htr3a': 'i', 'i6Pvalb': 'i', 'i5Pvalb': 'i',\n",
    "         'i4Pvalb': 'i'}   \n",
    "data_root = '../data/'\n",
    "dataset = Dataset(data_root, force_process=False) #specify data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3,4,5,6], [1,2,3,4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"0\" #specify gpu to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Split dataset and add correct labels for task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    if dataset.data_source == 'v1':\n",
    "        if dataset.labels_col == 'pop_name':\n",
    "            dataset.drop_dead_cells(cutoff=30) #each sample must have atleast 30 spikes\n",
    "            keepers = ['e5Rbp4', 'e23Cux2', 'i6Pvalb', 'e4Scnn1a', 'i23Pvalb', 'i23Htr3a',\n",
    "             'e4Rorb', 'e4other', 'i5Pvalb', 'i4Pvalb', 'i23Sst', 'i4Sst', 'e4Nr5a1',\n",
    "             'i1Htr3a', 'e5noRbp4', 'i6Sst', 'e6Ntsr1'] #cell classes identified by Louis as not being too quiet\n",
    "            dataset.drop_other_classes(classes_to_keep=keepers)\n",
    "            if k == '17celltypes':\n",
    "                pass #all necessary filtering done above\n",
    "            elif k == '13celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e23', 'i5Sst': 'i5Sst', 'i5Htr3a': 'i5Htr3a', 'e4Scnn1a': 'e4', 'e4Rorb': 'e4',\n",
    "                         'e4other': 'e4', 'e4Nr5a1': 'e4', 'i6Htr3a': 'i6Htr3a', 'i6Sst': 'i6Sst', 'e6Ntsr1': 'e6',\n",
    "                         'i23Pvalb': 'i23Pvalb', 'i23Htr3a': 'i23Htr3a', 'i1Htr3a': 'i1Htr3a', 'i4Sst': 'i4Sst', 'e5Rbp4': 'e5',\n",
    "                         'e5noRbp4': 'e5', 'i23Sst': 'i23Sst', 'i4Htr3a': 'i4Htr3a', 'i6Pvalb': 'i6Pvalb', 'i5Pvalb': 'i5Pvalb',\n",
    "                         'i4Pvalb': 'i4Pvalb'}        \n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '11celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'Cux2', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'Scnn1a', 'e4Rorb': 'Rorb',\n",
    "                         'e4other': 'other', 'e4Nr5a1': 'Nr5a1', 'i6Htr3a': 'Htr3a', 'i6Sst': 'Sst', 'e6Ntsr1': 'Ntsr1',\n",
    "                         'i23Pvalb': 'Pvalb', 'i23Htr3a': 'Htr3a', 'i1Htr3a': 'Htr3a', 'i4Sst': 'Sst', 'e5Rbp4': 'Rbp4',\n",
    "                         'e5noRbp4': 'noRbp4', 'i23Sst': 'Sst', 'i4Htr3a': 'Htr3a', 'i6Pvalb': 'Pvalb', 'i5Pvalb': 'Pvalb',\n",
    "                         'i4Pvalb': 'Pvalb'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '4celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'e', 'e4Rorb': 'e', 'e4other': 'e', \n",
    "                         'e4Nr5a1': 'e', 'i6Htr3a': 'Htr3a', 'i6Sst': 'Sst', 'e6Ntsr1': 'e', 'i23Pvalb': 'Pvalb', 'i23Htr3a': 'Htr3a',\n",
    "                         'i1Htr3a': 'Htr3a', 'i4Sst': 'Sst', 'e5Rbp4': 'e', 'e5noRbp4': 'e', 'i23Sst': 'Sst', 'i4Htr3a': 'Htr3a',\n",
    "                         'i6Pvalb': 'Pvalb', 'i5Pvalb': 'Pvalb', 'i4Pvalb': 'Pvalb'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '5layers':\n",
    "                aggr_dict = {'e23Cux2': '23', 'i5Sst': '5', 'i5Htr3a': '5', 'e4Scnn1a': '4', 'e4Rorb': '4', 'e4other': '4',\n",
    "                         'e4Nr5a1': '4', 'i6Htr3a': '6', 'i6Sst': '6', 'e6Ntsr1': '6', 'i23Pvalb': '23', 'i23Htr3a': '23',\n",
    "                         'i1Htr3a': '1', 'i4Sst': '4', 'e5Rbp4': '5', 'e5noRbp4': '5', 'i23Sst': '23', 'i4Htr3a': '4',\n",
    "                         'i6Pvalb': '6', 'i5Pvalb': '5', 'i4Pvalb': '4'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '2celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'i', 'i5Htr3a': 'i', 'e4Scnn1a': 'e', 'e4Rorb': 'e',\n",
    "                         'e4other': 'e', 'e4Nr5a1': 'e', 'i6Htr3a': 'i', 'i6Sst': 'i', 'e6Ntsr1': 'e',\n",
    "                         'i23Pvalb': 'i', 'i23Htr3a': 'i', 'i1Htr3a': 'i', 'i4Sst': 'i', 'e5Rbp4': 'e',\n",
    "                         'e5noRbp4': 'e', 'i23Sst': 'i', 'i4Htr3a': 'i', 'i6Pvalb': 'i', 'i5Pvalb': 'i',\n",
    "                         'i4Pvalb': 'i'}    \n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "\n",
    "    # Split into train/val/test sets\n",
    "    dataset.split_cell_train_val_test(test_size=test_size, val_size=val_size, seed=cell_split_seed)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, resample=False, window_size=None, log_interval=None):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if resample:\n",
    "            # Sample consecutive windows\n",
    "#             start = np.random.randint(0, int(data.shape[1] // 2))\n",
    "#             stop = start + int(data.shape[1] // 2)\n",
    "#             data = data[:,start:stop,:].sum(axis=1)\n",
    "            # Sample non consecutive windows\n",
    "            draws = np.random.choice(np.arange(0, data.shape[1]), size=data.shape[1])\n",
    "            data = data[:, draws, :].sum(axis=1)\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(model, device, loader, tag, resample=False, labels=dataset.cell_type_labels):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds = []\n",
    "    corrects = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            if tag == 'Train' and resample:\n",
    "                start = np.random.randint(0, int(data.shape[1] // 2))\n",
    "                stop = start + int(data.shape[1] // 2)\n",
    "                data = data[:,start:stop,:].sum(axis=1)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            pred_ = np.ndarray.flatten(pred.cpu().numpy())\n",
    "            targ_ = target.cpu().numpy()\n",
    "            preds.append(pred_)\n",
    "            corrects.append(targ_)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += len(target)\n",
    "    corrects = np.hstack(corrects)\n",
    "    preds = np.hstack(preds)\n",
    "    acc = f1_score(corrects,preds,average='macro')\n",
    "    cm = confusion_matrix(corrects,preds,normalize='true')\n",
    "    return cm,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Parameters\n",
      "Found processed pickle. Loading from '../data/processed/v1_dataset.pkl'.\n",
      "Loading Data\n",
      "Defining Transforms\n",
      "Filtering\n",
      "Transforming\n",
      "...Train\n",
      "...Test\n",
      "...Val\n",
      "Normalizing\n",
      "Retyping\n",
      "Preparing Loaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-efdef69dda64>:157: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_train, y_train = torch.FloatTensor(X_train), torch.LongTensor(y_train)\n",
      "<ipython-input-6-efdef69dda64>:159: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_test, y_test = torch.FloatTensor(X_test), torch.LongTensor(y_test)\n",
      "<ipython-input-6-efdef69dda64>:161: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_val, y_val = torch.FloatTensor(X_val), torch.LongTensor(y_val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running HP Idx 1\n",
      "0.4557955380390162 0.2668398624543523 0.27259454420306684\n",
      "0.47784991991628717 0.2780833085187512 0.28381739369872494\n",
      "0.48157767570720345 0.2907498159097917 0.2952154979817085\n",
      "0.4654374370041891 0.28446785532095714 0.2933372715012257\n",
      "0.48208891884366323 0.29171911671421025 0.29836949745876296\n",
      "0.507406210420959 0.29265803603152074 0.30674476479871393\n",
      "0.4766898508843505 0.3015126171920059 0.30490050933726165\n",
      "0.4770410724573675 0.2955601360836763 0.3009556785224405\n",
      "0.4891569172324222 0.29719834033349196 0.30663273185973466\n",
      "0.48563635082852097 0.29845426449023 0.30459279428539376\n",
      "Error saving CM: 1\n",
      "Loading Parameters\n",
      "Found processed pickle. Loading from '../data/processed/v1_dataset.pkl'.\n",
      "Loading Data\n",
      "Defining Transforms\n",
      "Filtering\n",
      "Transforming\n",
      "...Train\n",
      "...Test\n",
      "...Val\n",
      "Normalizing\n",
      "Retyping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-efdef69dda64>:157: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_train, y_train = torch.FloatTensor(X_train), torch.LongTensor(y_train)\n",
      "<ipython-input-6-efdef69dda64>:159: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_test, y_test = torch.FloatTensor(X_test), torch.LongTensor(y_test)\n",
      "<ipython-input-6-efdef69dda64>:161: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_val, y_val = torch.FloatTensor(X_val), torch.LongTensor(y_val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Loaders\n",
      "Running HP Idx 2\n",
      "0.5624641617786494 0.2820787636306288 0.28295056875068364\n",
      "0.5779451163265201 0.29394994266810487 0.3034234961850877\n",
      "0.595180034619076 0.28916482531676485 0.2935006841556329\n",
      "0.6130403713556898 0.31035343947932803 0.3135261110364766\n",
      "0.615385667870344 0.29852017826722355 0.3035551638383056\n",
      "0.623289920512903 0.316710063570498 0.3192517976540746\n",
      "0.6256559269205639 0.3102476091323883 0.3048186700285861\n",
      "0.6201324725476839 0.3214158727096928 0.31530134442335456\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-efdef69dda64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mtrain_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mval_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mtest_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a97050f22252>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, loader, tag, resample, labels)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mcorrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Train'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hp_df = pd.read_csv('hp_grid_resampling_v2.csv',sep='\\t') #this is a hyperparameter grid\n",
    "prev_dist_params = np.asarray([0]) #just empty initialization\n",
    "\n",
    "#FOR EVERY HYPERPARAMETER COMBINATION\n",
    "for index,row in hp_df.iterrows(): #just skipping the first row because I ran previously\n",
    "    \n",
    "    #LOAD THE HYPERPARAMETERS\n",
    "    print('Loading Parameters')\n",
    "    hpc = row['hp_idx']\n",
    "    #bin_size (below) applies to firing rate distributions only \n",
    "    k,distribution,cell_split_seed,bin_size = row['k'],row['distribution'],int(row['cell_split_seed']),row['bin_size']\n",
    "    isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step,fr_dist_bins_start,fr_dist_bins_stop,fr_dist_bins_step = [row['isi_dist_bins_start'],row['isi_dist_bins_stop'],row['isi_dist_bins_step'],row['fr_dist_bins_start'],row['fr_dist_bins_stop'],row['fr_dist_bins_step']]\n",
    "    lr,n_hiddens = row['lr'],row['n_hiddens']\n",
    "    n_hiddens = [int(nh) for nh in n_hiddens.rsplit(',')]\n",
    "    dist_params = [k,distribution,cell_split_seed,bin_size,isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step,fr_dist_bins_start,fr_dist_bins_stop,fr_dist_bins_step,]\n",
    "    \n",
    "    if len(list(set([isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step]))) > 1:\n",
    "        isi_dist_bins = list(np.arange(isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step))\n",
    "    else:\n",
    "        isi_dist_bins = isi_dist_bins_start\n",
    "    if len(list(set([fr_dist_bins_start,fr_dist_bins_stop,fr_dist_bins_step]))) > 1:    \n",
    "        fr_dist_bins = list(range(int(fr_dist_bins_start),int(fr_dist_bins_stop),int(fr_dist_bins_step)))\n",
    "    else:\n",
    "        fr_dist_bins = fr_dist_bins_start    \n",
    "    \n",
    "    if 'dropout_p' in hp_df.columns and 'weight_decay' in hp_df.columns:\n",
    "        dropout_p = row['dropout_p']\n",
    "        weight_decay = row['weight_decay']\n",
    "    else:\n",
    "        dropout_p = 0 #currently overfitting\n",
    "        weight_decay = 0\n",
    "        \n",
    "    if 'preaugmentation_perc1' in hp_df.columns and 'preaugmentation_perc2' in hp_df.columns:\n",
    "        preaugmentation_percs = [row['preaugmentation_perc1'],row['preaugmentation_perc2']]\n",
    "    else:\n",
    "        preaugmentation_percs = [0,0]    \n",
    "        \n",
    "    if 'augmentation_perc1' in hp_df.columns and 'augmentation_perc2' in hp_df.columns:\n",
    "        augmentation_percs = [row['augmentation_perc1'],row['augmentation_perc2']]\n",
    "    else:\n",
    "        augmentation_percs = [0,0]\n",
    "        \n",
    "    if 'window_size' in hp_df.columns:\n",
    "        window_size = row['window_size']\n",
    "        dist_params.append(window_size)\n",
    "    else : \n",
    "        window_size = 0\n",
    "\n",
    "    \n",
    "    #SET SOME HARDCODED HYPERPARAMETERS\n",
    "    batch_size = 1 #confusingly named- this is multiplied by base batch size to specify number of samples per class per batch (i.e. if base_batch_size = 20, num_classes = 5 and batch_size=1 , then 100 total samples per batch)\n",
    "    base_batch_size = 20 \n",
    "    n_class = int(''.join(filter(str.isdigit, k))) #determine number of classes from k in hp grid\n",
    "    cell_sample_seed = 1\n",
    "    test_size, val_size = 0.2,0.2   \n",
    "#     epochs = 50 #seems to be enough to converge\n",
    "    epochs = 100\n",
    "    threshold = 30 #minimum number of spikes per trial\n",
    "        \n",
    "    #IF THIS HYPERPARAMETER COMBO HAS DIFFERENT DATA (FEATURES OR CLASS LABELS) RELOAD AND RENORMALIZE THE DATA\n",
    "    if np.array_equal(np.asarray(dist_params),np.asarray(prev_dist_params))==False:\n",
    "        dataset = Dataset(data_root, force_process=False)\n",
    "        dataset.data_source = 'v1'\n",
    "        dataset.labels_col = 'pop_name'\n",
    "        dataset.num_trials_in_window = 33 \n",
    "        print('Loading Data')\n",
    "        dataset = load_data()\n",
    "        print('Defining Transforms')\n",
    "        fr_transform = FiringRates(window_size=3, bin_size=bin_size) #window_size is trial size in seconds\n",
    "        if distribution == 'ISI':\n",
    "            train_transform = ISIDistribution(bins=isi_dist_bins, min_isi=isi_dist_bins_start, \n",
    "                                              max_isi=isi_dist_bins_stop, \n",
    "                                              augmentation_percs=augmentation_percs, \n",
    "                                              preaugmentation_percs=preaugmentation_percs,\n",
    "                                              window_size=window_size)\n",
    "            val_test_transform = ISIDistribution(bins=isi_dist_bins, min_isi=isi_dist_bins_start, \n",
    "                                                 max_isi=isi_dist_bins_stop, augmentation_percs=[0,0], \n",
    "                                                 preaugmentation_percs=preaugmentation_percs,\n",
    "                                                window_size=window_size) #don't augment test/validation set  \n",
    "        elif distribution == 'FR':\n",
    "            data_transform = fr_transform\n",
    "            train_transform = data_transform\n",
    "            val_test_transform = data_transform\n",
    "        elif distribution == 'ISIFR':\n",
    "            isi_transform = ISIDistribution(bins=isi_dist_bins, min_isi=isi_dist_bins_start, \n",
    "                                            max_isi=isi_dist_bins_stop)\n",
    "            fr_transform = FiringRates(window_size=3, bin_size=bin_size)#window_size is trial size in seconds\n",
    "            data_transform = ConcatFeats(fr_transform, isi_transform)\n",
    "            train_transform = data_transform\n",
    "            val_test_transform = data_transform\n",
    "        \n",
    "        X_train_fr, y_train_fr = dataset.get_set('train', transform=fr_transform)\n",
    "        X_test_fr, y_test_fr = dataset.get_set('test', transform=fr_transform)    \n",
    "        X_val_fr, y_val_fr = dataset.get_set('val', transform=fr_transform)\n",
    "        \n",
    "        #SPLIT TRAIN/TEST/VAL SETS\n",
    "        #ENFORCE THRESHOLD ON MIN NUMBER OF SPIKES PER TRIAL\n",
    "        print('Filtering')\n",
    "        train_mask = X_train_fr.sum(axis=1) > threshold\n",
    "        test_mask = X_test_fr.sum(axis=1) > threshold\n",
    "        val_mask = X_val_fr.sum(axis=1) > threshold\n",
    "        print('Transforming')\n",
    "        print('...Train')\n",
    "        X_train, y_train = dataset.get_set('train')\n",
    "        X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "        if window_size > 0:\n",
    "            X_train, y_train = slice_data(X_train, y_train, window_size)\n",
    "        X_train = train_transform(np.array(X_train))\n",
    "\n",
    "        print('...Test')\n",
    "        X_test, y_test = dataset.get_set('test')\n",
    "        X_test, y_test = X_test[test_mask], y_test[test_mask]\n",
    "        if window_size > 0:\n",
    "            X_test, y_test = slice_data(X_test, y_test, window_size)\n",
    "        X_test = val_test_transform(np.array(X_test))\n",
    "        \n",
    "        print('...Val')\n",
    "        X_val, y_val = dataset.get_set('val')\n",
    "        X_val, y_val = X_val[val_mask], y_val[val_mask]\n",
    "        if window_size > 0:\n",
    "            X_val, y_val = slice_data(X_val, y_val, window_size)\n",
    "        X_val = val_test_transform(np.array(X_val))\n",
    "      \n",
    "        #NORMALIZE BASED ON TRAINING SET\n",
    "        print('Normalizing')\n",
    "        if len(X_train.shape) == 2:\n",
    "            train_scaler = StandardScaler()\n",
    "            train_scaler.fit(X_train.reshape(X_train.shape[0], -1))\n",
    "            print('...Train')\n",
    "            X_train = train_scaler.transform(X_train.reshape(X_train.shape[0], -1)).reshape(X_train.shape)\n",
    "            print('...Test')\n",
    "            X_test = train_scaler.transform(X_test.reshape(X_test.shape[0], -1)).reshape(X_test.shape)\n",
    "            print('...Val')\n",
    "            X_val = train_scaler.transform(X_val.reshape(X_val.shape[0], -1)).reshape(X_val.shape)\n",
    "        else : \n",
    "            scalers = {}\n",
    "            for i in range(X_train.shape[1]):\n",
    "                scalers[i] = StandardScaler()\n",
    "                X_train[:, i, :] = scalers[i].fit_transform(X_train[:, i, :]) \n",
    "\n",
    "            for i in range(X_test.shape[1]):\n",
    "                X_test[:, i, :] = scalers[i].transform(X_test[:, i, :]) \n",
    "                \n",
    "            for i in range(X_val.shape[1]):\n",
    "                X_val[:, i, :] = scalers[i].transform(X_val[:, i, :]) \n",
    "\n",
    "        # For scaling purposes ?\n",
    "        if window_size > 0:\n",
    "#             half_window = int(X_train.shape[1] // 2)\n",
    "#             X_test = X_test[:,0:half_window,:].sum(axis=1)\n",
    "#             X_val = X_val[0:,0:half_window,:].sum(axis=1)\n",
    "            X_test = X_test.sum(axis=1)\n",
    "            X_val = X_val.sum(axis=1)\n",
    "        \n",
    "        #RE-DTYPE SETS\n",
    "        print('Retyping')\n",
    "        X_train, y_train = torch.FloatTensor(X_train), torch.LongTensor(y_train)\n",
    "        train_dataset = TensorDataset(X_train,y_train)\n",
    "        X_test, y_test = torch.FloatTensor(X_test), torch.LongTensor(y_test)\n",
    "        test_dataset = TensorDataset(X_test,y_test)\n",
    "        X_val, y_val = torch.FloatTensor(X_val), torch.LongTensor(y_val)\n",
    "        val_dataset = TensorDataset(X_val,y_val)\n",
    "        \n",
    "        #SAMPLE WITH WEIGHTS TO COUNTER CLASS IMBALANCE\n",
    "        print('Preparing Loaders')\n",
    "        class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in y_train])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        samples_weight = samples_weight.double()\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=base_batch_size*dataset.num_cell_types, sampler=sampler)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=base_batch_size*dataset.num_cell_types, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=base_batch_size*dataset.num_cell_types, shuffle=True)\n",
    "        \n",
    "#         sys.exit()\n",
    "        \n",
    "    #RUN THE MODEL\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MLP(input_dims=X_train.shape[-1], n_hiddens=n_hiddens, n_class=n_class, dropout_p=dropout_p).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    train_accs, val_accs, test_accs = [],[],[] #used to save the f1s across iterations\n",
    "    resample = True if window_size > 0 else False\n",
    "    \n",
    "    print('Running HP Idx',hpc)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval=5, resample=resample)\n",
    "        train_cm, train_acc = test(model, device, train_loader, 'Train', resample=resample)\n",
    "        val_cm, val_acc = test(model, device, val_loader, 'Val')\n",
    "        test_cm, test_acc = test(model, device, test_loader, 'Test')\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        test_accs.append(test_acc)\n",
    "        if epoch%10==0:\n",
    "            print(train_acc,val_acc,test_acc)\n",
    "    \n",
    "    #SAVE THE F1 SCORES ACROSS ITERATIONS AND FINAL CONFUSION MATRIX\n",
    "    accs_df = pd.DataFrame(list(zip(train_accs, val_accs, test_accs)),columns=['train_acc','test_acc','val_acc'])\n",
    "    try:\n",
    "        accs_df.to_csv('hp_grid_f1s_{}.csv'.format(hpc),index=False)\n",
    "    except:\n",
    "        print('Error saving F1s:',hpc)\n",
    "    try:\n",
    "        np.save('ex_hp_grid_f1s_v2.npy')\n",
    "    except:\n",
    "        print('Error saving CM:',hpc)\n",
    "    \n",
    "    prev_dist_params = dist_params\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
