{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found processed pickle. Loading from '../data/processed/v1_dataset.pkl'.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "from src import Dataset, FiringRates, ISIDistribution\n",
    "from src.network import MLP\n",
    "\n",
    "aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'i', 'i5Htr3a': 'i', 'e4Scnn1a': 'e', 'e4Rorb': 'e',\n",
    "         'e4other': 'e', 'e4Nr5a1': 'e', 'i6Htr3a': 'i', 'i6Sst': 'i', 'e6Ntsr1': 'e',\n",
    "         'i23Pvalb': 'i', 'i23Htr3a': 'i', 'i1Htr3a': 'i', 'i4Sst': 'i', 'e5Rbp4': 'e',\n",
    "         'e5noRbp4': 'e', 'i23Sst': 'i', 'i4Htr3a': 'i', 'i6Pvalb': 'i', 'i5Pvalb': 'i',\n",
    "         'i4Pvalb': 'i'}        \n",
    "dataset = Dataset('../data', force_process=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Split dataset and add correct labels for task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    if dataset.data_source == 'v1':\n",
    "        if dataset.labels_col == 'pop_name':\n",
    "            dataset.drop_dead_cells(cutoff=30)\n",
    "            keepers = ['e5Rbp4', 'e23Cux2', 'i6Pvalb', 'e4Scnn1a', 'i23Pvalb', 'i23Htr3a',\n",
    "             'e4Rorb', 'e4other', 'i5Pvalb', 'i4Pvalb', 'i23Sst', 'i4Sst', 'e4Nr5a1',\n",
    "             'i1Htr3a', 'e5noRbp4', 'i6Sst', 'e6Ntsr1']\n",
    "            dataset.drop_other_classes(classes_to_keep=keepers)\n",
    "            print(np.unique(dataset.cell_type_ids))\n",
    "            if k == '17celltypes':\n",
    "                pass #all filtering done above\n",
    "            elif k == '13celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e23', 'i5Sst': 'i5Sst', 'i5Htr3a': 'i5Htr3a', 'e4Scnn1a': 'e4', 'e4Rorb': 'e4',\n",
    "                         'e4other': 'e4', 'e4Nr5a1': 'e4', 'i6Htr3a': 'i6Htr3a', 'i6Sst': 'i6Sst', 'e6Ntsr1': 'e6',\n",
    "                         'i23Pvalb': 'i23Pvalb', 'i23Htr3a': 'i23Htr3a', 'i1Htr3a': 'i1Htr3a', 'i4Sst': 'i4Sst', 'e5Rbp4': 'e5',\n",
    "                         'e5noRbp4': 'e5', 'i23Sst': 'i23Sst', 'i4Htr3a': 'i4Htr3a', 'i6Pvalb': 'i6Pvalb', 'i5Pvalb': 'i5Pvalb',\n",
    "                         'i4Pvalb': 'i4Pvalb'}        \n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '11celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'Cux2', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'Scnn1a', 'e4Rorb': 'Rorb',\n",
    "                         'e4other': 'other', 'e4Nr5a1': 'Nr5a1', 'i6Htr3a': 'Htr3a', 'i6Sst': 'Sst', 'e6Ntsr1': 'Ntsr1',\n",
    "                         'i23Pvalb': 'Pvalb', 'i23Htr3a': 'Htr3a', 'i1Htr3a': 'Htr3a', 'i4Sst': 'Sst', 'e5Rbp4': 'Rbp4',\n",
    "                         'e5noRbp4': 'noRbp4', 'i23Sst': 'Sst', 'i4Htr3a': 'Htr3a', 'i6Pvalb': 'Pvalb', 'i5Pvalb': 'Pvalb',\n",
    "                         'i4Pvalb': 'Pvalb'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '4celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'e', 'e4Rorb': 'e', 'e4other': 'e', \n",
    "                         'e4Nr5a1': 'e', 'i6Htr3a': 'Htr3a', 'i6Sst': 'Sst', 'e6Ntsr1': 'e', 'i23Pvalb': 'Pvalb', 'i23Htr3a': 'Htr3a',\n",
    "                         'i1Htr3a': 'Htr3a', 'i4Sst': 'Sst', 'e5Rbp4': 'e', 'e5noRbp4': 'e', 'i23Sst': 'Sst', 'i4Htr3a': 'Htr3a',\n",
    "                         'i6Pvalb': 'Pvalb', 'i5Pvalb': 'Pvalb', 'i4Pvalb': 'Pvalb'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '5layers':\n",
    "                aggr_dict = {'e23Cux2': '23', 'i5Sst': '5', 'i5Htr3a': '5', 'e4Scnn1a': '4', 'e4Rorb': '4', 'e4other': '4',\n",
    "                         'e4Nr5a1': '4', 'i6Htr3a': '6', 'i6Sst': '6', 'e6Ntsr1': '6', 'i23Pvalb': '23', 'i23Htr3a': '23',\n",
    "                         'i1Htr3a': '1', 'i4Sst': '4', 'e5Rbp4': '5', 'e5noRbp4': '5', 'i23Sst': '23', 'i4Htr3a': '4',\n",
    "                         'i6Pvalb': '6', 'i5Pvalb': '5', 'i4Pvalb': '4'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '2celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'i', 'i5Htr3a': 'i', 'e4Scnn1a': 'e', 'e4Rorb': 'e',\n",
    "                         'e4other': 'e', 'e4Nr5a1': 'e', 'i6Htr3a': 'i', 'i6Sst': 'i', 'e6Ntsr1': 'e',\n",
    "                         'i23Pvalb': 'i', 'i23Htr3a': 'i', 'i1Htr3a': 'i', 'i4Sst': 'i', 'e5Rbp4': 'e',\n",
    "                         'e5noRbp4': 'e', 'i23Sst': 'i', 'i4Htr3a': 'i', 'i6Pvalb': 'i', 'i5Pvalb': 'i',\n",
    "                         'i4Pvalb': 'i'}    \n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "\n",
    "    # Split into train/val/test sets\n",
    "    dataset.split_cell_train_val_test(test_size=test_size, val_size=val_size, seed=cell_split_seed)\n",
    "    #dataset.split_trial_train_val_test(test_size=0.2, val_size=0.2, temp=True, seed=1234)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=None):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        '''\n",
    "        if log_interval and batch_idx % log_interval == 0:\n",
    "            \n",
    "            print('Train Epoch: {} [({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, 100. * batch_idx / len(train_loader), loss.item()))\n",
    "                '''\n",
    "\n",
    "def test(model, device, loader, tag, labels=dataset.cell_type_labels):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds = []\n",
    "    corrects = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            pred_ = np.ndarray.flatten(pred.cpu().numpy())\n",
    "            targ_ = target.cpu().numpy()\n",
    "            preds.append(pred_)\n",
    "            corrects.append(targ_)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += len(target)\n",
    "    '''\n",
    "    print('{} set: Accuracy: {}/{} ({:.0f}%)'.format(tag,\n",
    "        correct, total,\n",
    "        100. * correct / total))\n",
    "        '''\n",
    "    corrects = np.hstack(corrects)\n",
    "    preds = np.hstack(preds)\n",
    "    acc = f1_score(corrects,preds,average='macro')\n",
    "    print('F1:',acc)\n",
    "    cm = confusion_matrix(corrects,preds,normalize='true')\n",
    "    return cm,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-a688049f2a53>:29: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if dist_params!=prev_dist_params:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Found processed pickle. Loading from '../data/processed/v1_dataset.pkl'.\n",
      "Load\n",
      "[ 0.  1.  2.  3.  4.  6.  7.  9. 10. 11. 12. 14. 15. 16. 18. 19. 20.]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a688049f2a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_trials_in_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Load'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mfr_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFiringRates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbin_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdistribution\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ISI'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-07251f793aa9>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                          \u001b[0;34m'e5noRbp4'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'e5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i23Sst'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'i23Sst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i4Htr3a'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'i4Htr3a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i6Pvalb'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'i6Pvalb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i5Pvalb'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'i5Pvalb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                          'i4Pvalb': 'i4Pvalb'}        \n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_cell_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggr_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'11celltypes'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 aggr_dict = {'e23Cux2': 'Cux2', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'Scnn1a', 'e4Rorb': 'Rorb',\n",
      "\u001b[0;32m/home/aschneider61/cell_type/src/data.py\u001b[0m in \u001b[0;36maggregate_cell_classes\u001b[0;34m(self, aggr_dict)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mnew_cell_type_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggregates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mnew_cell_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregation_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_type_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_type_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_cell_type_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_cell_type_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "hp_df = pd.read_csv('hp_grid_p6.csv') #this is a hyperparameter grid\n",
    "prev_dist_params = np.asarray([0]) #just empty initialization\n",
    "\n",
    "#FOR EVERY HYPERPARAMETER COMBINATION\n",
    "for index,row in hp_df.iterrows(): #just skipping the first row because I ran previously\n",
    "    \n",
    "    #LOAD THE HYPERPARAMETERS\n",
    "    hpc = row['hp_idx']\n",
    "    k,distribution,cell_split_seed,bin_size = row['k'],row['distribution'],row['cell_split_seed'],row['bin_size']\n",
    "    k = '13celltypes'\n",
    "    isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step,fr_dist_bins_start,fr_dist_bins_stop,fr_dist_bins_step = [row['isi_dist_bins_start'],row['isi_dist_bins_stop'],row['isi_dist_bins_step'],row['fr_dist_bins_start'],row['fr_dist_bins_stop'],row['fr_dist_bins_step']]\n",
    "    lr,n_hiddens = row['lr'],row['n_hiddens']\n",
    "    n_hiddens = [int(nh) for nh in n_hiddens.rsplit(',')]\n",
    "    dist_params = [k,distribution,cell_split_seed,bin_size,isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step,fr_dist_bins_start,fr_dist_bins_stop,fr_dist_bins_step,]\n",
    "    isi_dist_bins = list(np.arange(isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step))\n",
    "    fr_dist_bins = list(range(int(fr_dist_bins_start),int(fr_dist_bins_stop),int(fr_dist_bins_step)))\n",
    "    \n",
    "    #SET SOME HARDCODED HYPERPARAMETERS\n",
    "    batch_size = 1\n",
    "    n_class = int(''.join(filter(str.isdigit, k)))\n",
    "    cell_sample_seed = 1\n",
    "    test_size, val_size = 0.2,0.2 \n",
    "    \n",
    "    dropout_p = 0 #currently overfitting\n",
    "    epochs=50 #seems to be enough to converge\n",
    "    threshold = 30 #minimum number of spikes per trial\n",
    "    \n",
    "    #IF THIS HYPERPARAMETER COMBO HAS DIFFERENT DATA (FEATURES OR CLASS LABELS) RELOAD AND RENORMALIZE THE DATA\n",
    "    if dist_params!=prev_dist_params:\n",
    "        print(1)\n",
    "        dataset = Dataset('../data', force_process=False)\n",
    "        dataset.data_source = 'v1'\n",
    "        dataset.labels_col = 'pop_name'\n",
    "        dataset.num_trials_in_window = 33\n",
    "        print('Load')\n",
    "        dataset = load_data()\n",
    "        fr_transform = FiringRates(window_size=3, bin_size=bin_size)\n",
    "        if distribution == 'ISI':\n",
    "            data_transform = ISIDistribution(bins=isi_dist_bins, min_isi=0, max_isi=0.4)\n",
    "        elif distribution == 'FR':\n",
    "            data_transform = fr_transform\n",
    "        elif distribution == 'ISIFR':\n",
    "            isi_transform = ISIDistribution(bins=isi_dist_bins, min_isi=0, max_isi=0.4)\n",
    "            fr_transform = FiringRates(window_size=3, bin_size=bin_size)\n",
    "            data_transform = ConcatFeats(fr_transform, isi_transform)\n",
    "        print(2)\n",
    "        print(2.1)\n",
    "        X_train_fr, y_train_fr = dataset.get_set('train', transform=fr_transform)    \n",
    "        print(2.2)\n",
    "        X_test_fr, y_test_fr = dataset.get_set('test', transform=fr_transform)    \n",
    "        print(2.3)\n",
    "        X_val_fr, y_val_fr = dataset.get_set('val', transform=fr_transform)    \n",
    "        print(2.4)\n",
    "        train_mask = X_train_fr.sum(axis=1) > threshold\n",
    "        test_mask = X_test_fr.sum(axis=1) > threshold\n",
    "        val_mask = X_val_fr.sum(axis=1) > threshold\n",
    "        print(3)\n",
    "        print(3.1)\n",
    "        X_train, y_train = dataset.get_set('train',transform=data_transform)\n",
    "        X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "        print(3.2)\n",
    "        X_test, y_test = dataset.get_set('test',transform=data_transform)\n",
    "        X_test, y_test = X_test[test_mask], y_test[test_mask]\n",
    "        print(3.3)\n",
    "        X_val, y_val = dataset.get_set('val',transform=data_transform)\n",
    "        X_val, y_val = X_val[val_mask], y_val[val_mask]\n",
    "        print(4)\n",
    "        train_scaler = StandardScaler()\n",
    "        train_scaler.fit(X_train)\n",
    "        X_train = train_scaler.transform(X_train)\n",
    "        X_test = train_scaler.transform(X_test)\n",
    "        X_val = train_scaler.transform(X_val)\n",
    "        print(5)\n",
    "        X_train, y_train = torch.FloatTensor(X_train), torch.LongTensor(y_train)\n",
    "        train_dataset = TensorDataset(X_train,y_train)\n",
    "        X_test, y_test = torch.FloatTensor(X_test), torch.LongTensor(y_test)\n",
    "        test_dataset = TensorDataset(X_test,y_test)\n",
    "        X_val, y_val = torch.FloatTensor(X_val), torch.LongTensor(y_val)\n",
    "        val_dataset = TensorDataset(X_val,y_val)\n",
    "        print(6)\n",
    "        class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in y_train])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        samples_weight = samples_weight.double()\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        print(7)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=20*dataset.num_cell_types, shuffle=True, sampler=sampler)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=20*dataset.num_cell_type, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=20*dataset.num_cell_type, shuffle=True)\n",
    "        print(8)\n",
    "    #RUN THE MODEL\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MLP(input_dims=train_dataset.num_bins, n_hiddens=n_hiddens, n_class=n_class, dropout_p=dropout_p).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_accs, val_accs, test_accs = [],[],[] #used to save the f1s across iterations\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval=5)\n",
    "        train_cm, train_acc = test(model, device, train_loader, 'Train')\n",
    "        val_cm, val_acc = test(model, device, val_loader, 'Val')\n",
    "        test_cm, test_acc = test(model, device, test_loader, 'Test')\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        test_accs.append(test_acc)\n",
    "    \n",
    "    #SAVE THE F1 SCORES ACROSS ITERATIONS AND FINAL CONFUSION MATRIX\n",
    "    accs_df = pd.DataFrame(list(zip(train_accs, val_accs, test_accs)),columns=['train_acc','test_acc','val_acc'])\n",
    "    accs_df.to_csv('{}_hp{}_f1s_v2.csv'.format(distribution,str(hpc)),index=False)\n",
    "    np.save('{}_hp{}_cms_v2.npy'.format(distribution,str(hpc)),np.dstack([train_cm,val_cm,test_cm]))\n",
    "    prev_dist_params = dist_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
