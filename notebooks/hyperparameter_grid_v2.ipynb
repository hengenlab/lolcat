{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found processed pickle. Loading from '../data/processed/v1_dataset.pkl'.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "from src import Dataset, FiringRates, ISIDistribution\n",
    "from src.network import MLP\n",
    "from src.augmentations import slice_data\n",
    "\n",
    "aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'i', 'i5Htr3a': 'i', 'e4Scnn1a': 'e', 'e4Rorb': 'e',\n",
    "         'e4other': 'e', 'e4Nr5a1': 'e', 'i6Htr3a': 'i', 'i6Sst': 'i', 'e6Ntsr1': 'e',\n",
    "         'i23Pvalb': 'i', 'i23Htr3a': 'i', 'i1Htr3a': 'i', 'i4Sst': 'i', 'e5Rbp4': 'e',\n",
    "         'e5noRbp4': 'e', 'i23Sst': 'i', 'i4Htr3a': 'i', 'i6Pvalb': 'i', 'i5Pvalb': 'i',\n",
    "         'i4Pvalb': 'i'}   \n",
    "data_root = '../data/'\n",
    "dataset = Dataset(data_root, force_process=False) #specify data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3,4,5,6], [1,2,3,4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"0\" #specify gpu to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Split dataset and add correct labels for task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    if dataset.data_source == 'v1':\n",
    "        if dataset.labels_col == 'pop_name':\n",
    "            dataset.drop_dead_cells(cutoff=30) #each sample must have atleast 30 spikes\n",
    "            keepers = ['e5Rbp4', 'e23Cux2', 'i6Pvalb', 'e4Scnn1a', 'i23Pvalb', 'i23Htr3a',\n",
    "             'e4Rorb', 'e4other', 'i5Pvalb', 'i4Pvalb', 'i23Sst', 'i4Sst', 'e4Nr5a1',\n",
    "             'i1Htr3a', 'e5noRbp4', 'i6Sst', 'e6Ntsr1'] #cell classes identified by Louis as not being too quiet\n",
    "            dataset.drop_other_classes(classes_to_keep=keepers)\n",
    "            if k == '17celltypes':\n",
    "                pass #all necessary filtering done above\n",
    "            elif k == '13celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e23', 'i5Sst': 'i5Sst', 'i5Htr3a': 'i5Htr3a', 'e4Scnn1a': 'e4', 'e4Rorb': 'e4',\n",
    "                         'e4other': 'e4', 'e4Nr5a1': 'e4', 'i6Htr3a': 'i6Htr3a', 'i6Sst': 'i6Sst', 'e6Ntsr1': 'e6',\n",
    "                         'i23Pvalb': 'i23Pvalb', 'i23Htr3a': 'i23Htr3a', 'i1Htr3a': 'i1Htr3a', 'i4Sst': 'i4Sst', 'e5Rbp4': 'e5',\n",
    "                         'e5noRbp4': 'e5', 'i23Sst': 'i23Sst', 'i4Htr3a': 'i4Htr3a', 'i6Pvalb': 'i6Pvalb', 'i5Pvalb': 'i5Pvalb',\n",
    "                         'i4Pvalb': 'i4Pvalb'}        \n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '11celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'Cux2', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'Scnn1a', 'e4Rorb': 'Rorb',\n",
    "                         'e4other': 'other', 'e4Nr5a1': 'Nr5a1', 'i6Htr3a': 'Htr3a', 'i6Sst': 'Sst', 'e6Ntsr1': 'Ntsr1',\n",
    "                         'i23Pvalb': 'Pvalb', 'i23Htr3a': 'Htr3a', 'i1Htr3a': 'Htr3a', 'i4Sst': 'Sst', 'e5Rbp4': 'Rbp4',\n",
    "                         'e5noRbp4': 'noRbp4', 'i23Sst': 'Sst', 'i4Htr3a': 'Htr3a', 'i6Pvalb': 'Pvalb', 'i5Pvalb': 'Pvalb',\n",
    "                         'i4Pvalb': 'Pvalb'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '4celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'Sst', 'i5Htr3a': 'Htr3a', 'e4Scnn1a': 'e', 'e4Rorb': 'e', 'e4other': 'e', \n",
    "                         'e4Nr5a1': 'e', 'i6Htr3a': 'Htr3a', 'i6Sst': 'Sst', 'e6Ntsr1': 'e', 'i23Pvalb': 'Pvalb', 'i23Htr3a': 'Htr3a',\n",
    "                         'i1Htr3a': 'Htr3a', 'i4Sst': 'Sst', 'e5Rbp4': 'e', 'e5noRbp4': 'e', 'i23Sst': 'Sst', 'i4Htr3a': 'Htr3a',\n",
    "                         'i6Pvalb': 'Pvalb', 'i5Pvalb': 'Pvalb', 'i4Pvalb': 'Pvalb'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '5layers':\n",
    "                aggr_dict = {'e23Cux2': '23', 'i5Sst': '5', 'i5Htr3a': '5', 'e4Scnn1a': '4', 'e4Rorb': '4', 'e4other': '4',\n",
    "                         'e4Nr5a1': '4', 'i6Htr3a': '6', 'i6Sst': '6', 'e6Ntsr1': '6', 'i23Pvalb': '23', 'i23Htr3a': '23',\n",
    "                         'i1Htr3a': '1', 'i4Sst': '4', 'e5Rbp4': '5', 'e5noRbp4': '5', 'i23Sst': '23', 'i4Htr3a': '4',\n",
    "                         'i6Pvalb': '6', 'i5Pvalb': '5', 'i4Pvalb': '4'}\n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "            elif k == '2celltypes':\n",
    "                aggr_dict = {'e23Cux2': 'e', 'i5Sst': 'i', 'i5Htr3a': 'i', 'e4Scnn1a': 'e', 'e4Rorb': 'e',\n",
    "                         'e4other': 'e', 'e4Nr5a1': 'e', 'i6Htr3a': 'i', 'i6Sst': 'i', 'e6Ntsr1': 'e',\n",
    "                         'i23Pvalb': 'i', 'i23Htr3a': 'i', 'i1Htr3a': 'i', 'i4Sst': 'i', 'e5Rbp4': 'e',\n",
    "                         'e5noRbp4': 'e', 'i23Sst': 'i', 'i4Htr3a': 'i', 'i6Pvalb': 'i', 'i5Pvalb': 'i',\n",
    "                         'i4Pvalb': 'i'}    \n",
    "                dataset.aggregate_cell_classes(aggr_dict)\n",
    "\n",
    "    # Split into train/val/test sets\n",
    "    dataset.split_cell_train_val_test(test_size=test_size, val_size=val_size, seed=cell_split_seed)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, resample=False, window_size=None, log_interval=None):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if resample:\n",
    "            # Sample consecutive windows\n",
    "#             start = np.random.randint(0, int(data.shape[1] // 2))\n",
    "#             stop = start + int(data.shape[1] // 2)\n",
    "#             data = data[:,start:stop,:].sum(axis=1)\n",
    "            # Sample non consecutive windows\n",
    "            draws = np.random.choice(np.arange(0, data.shape[1]), size=data.shape[1])\n",
    "            data = data[:, draws, :].sum(axis=1)\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(model, device, loader, tag, resample=False, labels=dataset.cell_type_labels):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds = []\n",
    "    corrects = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            if tag == 'Train' and resample:\n",
    "                start = np.random.randint(0, int(data.shape[1] // 2))\n",
    "                stop = start + int(data.shape[1] // 2)\n",
    "                data = data[:,start:stop,:].sum(axis=1)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            pred_ = np.ndarray.flatten(pred.cpu().numpy())\n",
    "            targ_ = target.cpu().numpy()\n",
    "            preds.append(pred_)\n",
    "            corrects.append(targ_)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += len(target)\n",
    "    corrects = np.hstack(corrects)\n",
    "    preds = np.hstack(preds)\n",
    "    acc = f1_score(corrects,preds,average='macro')\n",
    "    cm = confusion_matrix(corrects,preds,normalize='true')\n",
    "    return cm,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Parameters\n",
      "Found processed pickle. Loading from '../data/processed/v1_dataset.pkl'.\n",
      "Loading Data\n",
      "Defining Transforms\n",
      "Filtering\n",
      "Transforming\n",
      "...Train\n",
      "...Test\n",
      "...Val\n",
      "Normalizing\n",
      "Retyping\n",
      "Preparing Loaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-efdef69dda64>:157: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_train, y_train = torch.FloatTensor(X_train), torch.LongTensor(y_train)\n",
      "<ipython-input-6-efdef69dda64>:159: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_test, y_test = torch.FloatTensor(X_test), torch.LongTensor(y_test)\n",
      "<ipython-input-6-efdef69dda64>:161: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_val, y_val = torch.FloatTensor(X_val), torch.LongTensor(y_val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running HP Idx 1\n",
      "0.4692716419669442 0.27418184215622016 0.2791614733428191\n",
      "0.46251683425892615 0.28926659993410303 0.29355781000339987\n",
      "0.474255519151682 0.2839141905981119 0.2871349363105714\n",
      "0.4623804378018788 0.2874058801775061 0.29340804499148554\n",
      "0.479426246417182 0.29863561256238474 0.3031238118750418\n",
      "0.49112025040224433 0.2942162505931409 0.3117266510526591\n",
      "0.4745287588579764 0.28900048935877953 0.30295852913588983\n",
      "0.48006949817442335 0.2899069142092447 0.3087678604803471\n",
      "0.46967091216481505 0.30054124439665225 0.3087100400557008\n",
      "0.4797021676994801 0.29282373375817417 0.2979832738217002\n",
      "Error saving CM: 1\n",
      "Loading Parameters\n",
      "Found processed pickle. Loading from '../data/processed/v1_dataset.pkl'.\n",
      "Loading Data\n",
      "Defining Transforms\n",
      "Filtering\n",
      "Transforming\n",
      "...Train\n",
      "...Test\n",
      "...Val\n",
      "Normalizing\n",
      "Retyping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-efdef69dda64>:157: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_train, y_train = torch.FloatTensor(X_train), torch.LongTensor(y_train)\n",
      "<ipython-input-6-efdef69dda64>:159: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_test, y_test = torch.FloatTensor(X_test), torch.LongTensor(y_test)\n",
      "<ipython-input-6-efdef69dda64>:161: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_val, y_val = torch.FloatTensor(X_val), torch.LongTensor(y_val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Loaders\n",
      "Running HP Idx 2\n",
      "0.5537518472151524 0.2795844433780546 0.27765833892380215\n",
      "0.5859749316344198 0.29041195591150465 0.30267613016225564\n",
      "0.5948147985997393 0.30668480013491917 0.3081037972393929\n",
      "0.6163487041232443 0.2963673916164133 0.3078239277432949\n",
      "0.6066719527941928 0.3024058911064579 0.3154436921413863\n",
      "0.6281973083648563 0.3029257841945574 0.3128719726530387\n",
      "0.6263696634752031 0.31223592077673495 0.320554785040143\n",
      "0.6086611521429475 0.3030644795762543 0.32044715695260606\n",
      "0.6302499492288008 0.30106366920755123 0.31117887286587065\n",
      "0.6130763828902765 0.2987731604650512 0.3188632195507348\n",
      "Error saving CM: 2\n",
      "Loading Parameters\n",
      "Found processed pickle. Loading from '../data/processed/v1_dataset.pkl'.\n",
      "Loading Data\n",
      "Defining Transforms\n",
      "Filtering\n",
      "Transforming\n",
      "...Train\n",
      "...Test\n",
      "...Val\n",
      "Normalizing\n",
      "Retyping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-efdef69dda64>:157: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_train, y_train = torch.FloatTensor(X_train), torch.LongTensor(y_train)\n",
      "<ipython-input-6-efdef69dda64>:159: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_test, y_test = torch.FloatTensor(X_test), torch.LongTensor(y_test)\n",
      "<ipython-input-6-efdef69dda64>:161: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  X_val, y_val = torch.FloatTensor(X_val), torch.LongTensor(y_val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Loaders\n",
      "Running HP Idx 3\n",
      "0.6531952513912349 0.2918606735832826 0.30076621404136583\n",
      "0.6789030842316508 0.2932467511874659 0.3180810575116938\n",
      "0.6799871119200543 0.2874269301614111 0.28690889281100307\n",
      "0.6832217611403206 0.28981346060413915 0.2987156153424817\n",
      "0.6997597857671882 0.2930400661578954 0.3157802550346904\n",
      "0.6980372349060395 0.29686711333330906 0.300784400569928\n",
      "0.7031547272391686 0.29481087689013313 0.28901700365530264\n",
      "0.7262099296677893 0.2903167908862487 0.30200833414351463\n",
      "0.7270000040441111 0.2954323061532475 0.29920721142839\n",
      "0.7383991213892585 0.2941120310028662 0.30408383195832345\n",
      "Error saving CM: 3\n",
      "Loading Parameters\n",
      "Running HP Idx 3\n",
      "0.5933120853254823 0.2941112172633319 0.29372254976467266\n",
      "0.5902315250813943 0.29951186220530523 0.3056487715422939\n",
      "0.5846526245354668 0.31142108632664733 0.3201706117127586\n",
      "0.5846227837847355 0.3106012428546102 0.31058913723632625\n",
      "0.5634154920124205 0.3061085945760241 0.30919460911278696\n",
      "0.5594381937136714 0.3014867191630592 0.3139047274404069\n",
      "0.5772691380001331 0.3101914167549924 0.31422091855274964\n",
      "0.5544479903605813 0.3101676094245727 0.31648795981481526\n",
      "0.5399713240122247 0.3092180895739221 0.31312626159270124\n",
      "0.5509742325690796 0.31048072849773856 0.31757426973657116\n",
      "Error saving CM: 3\n"
     ]
    }
   ],
   "source": [
    "hp_df = pd.read_csv('hp_grid_resampling_v2.csv',sep='\\t') #this is a hyperparameter grid\n",
    "prev_dist_params = np.asarray([0]) #just empty initialization\n",
    "\n",
    "#FOR EVERY HYPERPARAMETER COMBINATION\n",
    "for index,row in hp_df.iterrows(): #just skipping the first row because I ran previously\n",
    "    \n",
    "    #LOAD THE HYPERPARAMETERS\n",
    "    print('Loading Parameters')\n",
    "    hpc = row['hp_idx']\n",
    "    #bin_size (below) applies to firing rate distributions only \n",
    "    k,distribution,cell_split_seed,bin_size = row['k'],row['distribution'],int(row['cell_split_seed']),row['bin_size']\n",
    "    isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step,fr_dist_bins_start,fr_dist_bins_stop,fr_dist_bins_step = [row['isi_dist_bins_start'],row['isi_dist_bins_stop'],row['isi_dist_bins_step'],row['fr_dist_bins_start'],row['fr_dist_bins_stop'],row['fr_dist_bins_step']]\n",
    "    lr,n_hiddens = row['lr'],row['n_hiddens']\n",
    "    n_hiddens = [int(nh) for nh in n_hiddens.rsplit(',')]\n",
    "    dist_params = [k,distribution,cell_split_seed,bin_size,isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step,fr_dist_bins_start,fr_dist_bins_stop,fr_dist_bins_step,]\n",
    "    \n",
    "    if len(list(set([isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step]))) > 1:\n",
    "        isi_dist_bins = list(np.arange(isi_dist_bins_start,isi_dist_bins_stop,isi_dist_bins_step))\n",
    "    else:\n",
    "        isi_dist_bins = isi_dist_bins_start\n",
    "    if len(list(set([fr_dist_bins_start,fr_dist_bins_stop,fr_dist_bins_step]))) > 1:    \n",
    "        fr_dist_bins = list(range(int(fr_dist_bins_start),int(fr_dist_bins_stop),int(fr_dist_bins_step)))\n",
    "    else:\n",
    "        fr_dist_bins = fr_dist_bins_start    \n",
    "    \n",
    "    if 'dropout_p' in hp_df.columns and 'weight_decay' in hp_df.columns:\n",
    "        dropout_p = row['dropout_p']\n",
    "        weight_decay = row['weight_decay']\n",
    "    else:\n",
    "        dropout_p = 0 #currently overfitting\n",
    "        weight_decay = 0\n",
    "        \n",
    "    if 'preaugmentation_perc1' in hp_df.columns and 'preaugmentation_perc2' in hp_df.columns:\n",
    "        preaugmentation_percs = [row['preaugmentation_perc1'],row['preaugmentation_perc2']]\n",
    "    else:\n",
    "        preaugmentation_percs = [0,0]    \n",
    "        \n",
    "    if 'augmentation_perc1' in hp_df.columns and 'augmentation_perc2' in hp_df.columns:\n",
    "        augmentation_percs = [row['augmentation_perc1'],row['augmentation_perc2']]\n",
    "    else:\n",
    "        augmentation_percs = [0,0]\n",
    "        \n",
    "    if 'window_size' in hp_df.columns:\n",
    "        window_size = row['window_size']\n",
    "        dist_params.append(window_size)\n",
    "    else : \n",
    "        window_size = 0\n",
    "\n",
    "    \n",
    "    #SET SOME HARDCODED HYPERPARAMETERS\n",
    "    batch_size = 1 #confusingly named- this is multiplied by base batch size to specify number of samples per class per batch (i.e. if base_batch_size = 20, num_classes = 5 and batch_size=1 , then 100 total samples per batch)\n",
    "    base_batch_size = 20 \n",
    "    n_class = int(''.join(filter(str.isdigit, k))) #determine number of classes from k in hp grid\n",
    "    cell_sample_seed = 1\n",
    "    test_size, val_size = 0.2,0.2   \n",
    "#     epochs = 50 #seems to be enough to converge\n",
    "    epochs = 100\n",
    "    threshold = 30 #minimum number of spikes per trial\n",
    "        \n",
    "    #IF THIS HYPERPARAMETER COMBO HAS DIFFERENT DATA (FEATURES OR CLASS LABELS) RELOAD AND RENORMALIZE THE DATA\n",
    "    if np.array_equal(np.asarray(dist_params),np.asarray(prev_dist_params))==False:\n",
    "        dataset = Dataset(data_root, force_process=False)\n",
    "        dataset.data_source = 'v1'\n",
    "        dataset.labels_col = 'pop_name'\n",
    "        dataset.num_trials_in_window = 33 \n",
    "        print('Loading Data')\n",
    "        dataset = load_data()\n",
    "        print('Defining Transforms')\n",
    "        fr_transform = FiringRates(window_size=3, bin_size=bin_size) #window_size is trial size in seconds\n",
    "        if distribution == 'ISI':\n",
    "            train_transform = ISIDistribution(bins=isi_dist_bins, min_isi=isi_dist_bins_start, \n",
    "                                              max_isi=isi_dist_bins_stop, \n",
    "                                              augmentation_percs=augmentation_percs, \n",
    "                                              preaugmentation_percs=preaugmentation_percs,\n",
    "                                              window_size=window_size)\n",
    "            val_test_transform = ISIDistribution(bins=isi_dist_bins, min_isi=isi_dist_bins_start, \n",
    "                                                 max_isi=isi_dist_bins_stop, augmentation_percs=[0,0], \n",
    "                                                 preaugmentation_percs=preaugmentation_percs,\n",
    "                                                window_size=window_size) #don't augment test/validation set  \n",
    "        elif distribution == 'FR':\n",
    "            data_transform = fr_transform\n",
    "            train_transform = data_transform\n",
    "            val_test_transform = data_transform\n",
    "        elif distribution == 'ISIFR':\n",
    "            isi_transform = ISIDistribution(bins=isi_dist_bins, min_isi=isi_dist_bins_start, \n",
    "                                            max_isi=isi_dist_bins_stop)\n",
    "            fr_transform = FiringRates(window_size=3, bin_size=bin_size)#window_size is trial size in seconds\n",
    "            data_transform = ConcatFeats(fr_transform, isi_transform)\n",
    "            train_transform = data_transform\n",
    "            val_test_transform = data_transform\n",
    "        \n",
    "        X_train_fr, y_train_fr = dataset.get_set('train', transform=fr_transform)\n",
    "        X_test_fr, y_test_fr = dataset.get_set('test', transform=fr_transform)    \n",
    "        X_val_fr, y_val_fr = dataset.get_set('val', transform=fr_transform)\n",
    "        \n",
    "        #SPLIT TRAIN/TEST/VAL SETS\n",
    "        #ENFORCE THRESHOLD ON MIN NUMBER OF SPIKES PER TRIAL\n",
    "        print('Filtering')\n",
    "        train_mask = X_train_fr.sum(axis=1) > threshold\n",
    "        test_mask = X_test_fr.sum(axis=1) > threshold\n",
    "        val_mask = X_val_fr.sum(axis=1) > threshold\n",
    "        print('Transforming')\n",
    "        print('...Train')\n",
    "        X_train, y_train = dataset.get_set('train')\n",
    "        X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "        if window_size > 0:\n",
    "            X_train, y_train = slice_data(X_train, y_train, window_size)\n",
    "        X_train = train_transform(np.array(X_train))\n",
    "\n",
    "        print('...Test')\n",
    "        X_test, y_test = dataset.get_set('test')\n",
    "        X_test, y_test = X_test[test_mask], y_test[test_mask]\n",
    "        if window_size > 0:\n",
    "            X_test, y_test = slice_data(X_test, y_test, window_size)\n",
    "        X_test = val_test_transform(np.array(X_test))\n",
    "        \n",
    "        print('...Val')\n",
    "        X_val, y_val = dataset.get_set('val')\n",
    "        X_val, y_val = X_val[val_mask], y_val[val_mask]\n",
    "        if window_size > 0:\n",
    "            X_val, y_val = slice_data(X_val, y_val, window_size)\n",
    "        X_val = val_test_transform(np.array(X_val))\n",
    "      \n",
    "        #NORMALIZE BASED ON TRAINING SET\n",
    "        print('Normalizing')\n",
    "        if len(X_train.shape) == 2:\n",
    "            train_scaler = StandardScaler()\n",
    "            train_scaler.fit(X_train.reshape(X_train.shape[0], -1))\n",
    "            print('...Train')\n",
    "            X_train = train_scaler.transform(X_train.reshape(X_train.shape[0], -1)).reshape(X_train.shape)\n",
    "            print('...Test')\n",
    "            X_test = train_scaler.transform(X_test.reshape(X_test.shape[0], -1)).reshape(X_test.shape)\n",
    "            print('...Val')\n",
    "            X_val = train_scaler.transform(X_val.reshape(X_val.shape[0], -1)).reshape(X_val.shape)\n",
    "        else : \n",
    "            scalers = {}\n",
    "            for i in range(X_train.shape[1]):\n",
    "                scalers[i] = StandardScaler()\n",
    "                X_train[:, i, :] = scalers[i].fit_transform(X_train[:, i, :]) \n",
    "\n",
    "            for i in range(X_test.shape[1]):\n",
    "                X_test[:, i, :] = scalers[i].transform(X_test[:, i, :]) \n",
    "                \n",
    "            for i in range(X_val.shape[1]):\n",
    "                X_val[:, i, :] = scalers[i].transform(X_val[:, i, :]) \n",
    "\n",
    "        # For scaling purposes ?\n",
    "        if window_size > 0:\n",
    "#             half_window = int(X_train.shape[1] // 2)\n",
    "#             X_test = X_test[:,0:half_window,:].sum(axis=1)\n",
    "#             X_val = X_val[0:,0:half_window,:].sum(axis=1)\n",
    "            X_test = X_test.sum(axis=1)\n",
    "            X_val = X_val.sum(axis=1)\n",
    "        \n",
    "        #RE-DTYPE SETS\n",
    "        print('Retyping')\n",
    "        X_train, y_train = torch.FloatTensor(X_train), torch.LongTensor(y_train)\n",
    "        train_dataset = TensorDataset(X_train,y_train)\n",
    "        X_test, y_test = torch.FloatTensor(X_test), torch.LongTensor(y_test)\n",
    "        test_dataset = TensorDataset(X_test,y_test)\n",
    "        X_val, y_val = torch.FloatTensor(X_val), torch.LongTensor(y_val)\n",
    "        val_dataset = TensorDataset(X_val,y_val)\n",
    "        \n",
    "        #SAMPLE WITH WEIGHTS TO COUNTER CLASS IMBALANCE\n",
    "        print('Preparing Loaders')\n",
    "        class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in y_train])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        samples_weight = samples_weight.double()\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=base_batch_size*dataset.num_cell_types, sampler=sampler)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=base_batch_size*dataset.num_cell_types, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=base_batch_size*dataset.num_cell_types, shuffle=True)\n",
    "        \n",
    "#         sys.exit()\n",
    "        \n",
    "    #RUN THE MODEL\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MLP(input_dims=X_train.shape[-1], n_hiddens=n_hiddens, n_class=n_class, dropout_p=dropout_p).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    train_accs, val_accs, test_accs = [],[],[] #used to save the f1s across iterations\n",
    "    resample = True if window_size > 0 else False\n",
    "    \n",
    "    print('Running HP Idx',hpc)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval=5, resample=resample)\n",
    "        train_cm, train_acc = test(model, device, train_loader, 'Train', resample=resample)\n",
    "        val_cm, val_acc = test(model, device, val_loader, 'Val')\n",
    "        test_cm, test_acc = test(model, device, test_loader, 'Test')\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        test_accs.append(test_acc)\n",
    "        if epoch%10==0:\n",
    "            print(train_acc,val_acc,test_acc)\n",
    "    \n",
    "    #SAVE THE F1 SCORES ACROSS ITERATIONS AND FINAL CONFUSION MATRIX\n",
    "    accs_df = pd.DataFrame(list(zip(train_accs, val_accs, test_accs)),columns=['train_acc','test_acc','val_acc'])\n",
    "    try:\n",
    "        accs_df.to_csv('hp_grid_f1s_{}.csv'.format(hpc),index=False)\n",
    "    except:\n",
    "        print('Error saving F1s:',hpc)\n",
    "    try:\n",
    "        np.save('ex_hp_grid_f1s_v2.npy')\n",
    "    except:\n",
    "        print('Error saving CM:',hpc)\n",
    "    \n",
    "    prev_dist_params = dist_params\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
